{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用昨天的卡通資料集來練GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, UpSampling2D, Dense, Flatten, Input, BatchNormalization, Reshape, LeakyReLU, Conv2DTranspose, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, folder_path, img_size):\n",
    "        self.folder_path = folder_path\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.path_list = glob(folder_path) # 讀取資料夾全部圖片路徑\n",
    "        assert len(self.path_list) > 0, 'path not existed!'\n",
    "    \n",
    "    def __imread(self, img_path):\n",
    "        '''讀取圖片'''\n",
    "        return np.array(Image.open(img_path).convert('RGB').resize(self.img_size[:-1]))\n",
    "    \n",
    "    def sampling_data(self, batch_size, shuffle=True):\n",
    "        img_path_list = self.path_list\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(img_path_list)\n",
    "            \n",
    "        for batch_idx in range(0, len(img_path_list), batch_size):\n",
    "            path_set = img_path_list[batch_idx : batch_idx + batch_size]\n",
    "            \n",
    "            # 預設空間，避免 append很慢\n",
    "            img_set = np.zeros((len(path_set),) + self.img_size)\n",
    "            for img_idx, path in enumerate(path_set):\n",
    "                img_set[img_idx] = self.__imread(path)\n",
    "            \n",
    "            # 127.5是255的一半，一到負一之間\n",
    "            img_set = img_set / 127.5 - 1\n",
    "            # 暫停\n",
    "            yield img_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader('../0709/Preview/cartoon/*.png', (32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x210921c99b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_one = next(data.sampling_data(1))\n",
    "# 強制變回 0到 1\n",
    "plt.imshow(next_one[0]*.5+.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_one[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, noise_dim, img_size=(64, 64, 3)):\n",
    "        self.noise_dim = noise_dim # noise_dim = 雜訊維度\n",
    "        self.img_size = img_size # img_size = 圖片大小\n",
    "        self.dataloader = DataLoader('../0709/Preview/cartoon/*.png', self.img_size)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise_input = Input(shape=(self.noise_dim,))\n",
    "        # TODO: Build generator\n",
    "        # 首先，將輸入轉換為16x16 128通道的feature map\n",
    "        x = Dense(128 * 4 * 4)(noise_input)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Reshape((4, 4, 128))(x)\n",
    "\n",
    "        # 然後，添加捲積層\n",
    "        x = Conv2DTranspose(1024, 5, strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        # 上取樣至 16 x 16\n",
    "        x = Conv2DTranspose(512, 4, strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        # 新增更多的卷積層\n",
    "        x = Conv2DTranspose(256, 5, strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Conv2DTranspose(128, 5, strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "\n",
    "        # 生成一個 64x64 3-channel 的feature map\n",
    "        img = Conv2D(self.img_size[2], 7, activation='tanh', padding='same')(x)\n",
    "        generator = Model(noise_input, img)\n",
    "#         generator.summary()\n",
    "        return generator\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_input = Input(shape=self.img_size)\n",
    "        # TODO: Build generator\n",
    "        x = Conv2D(64, 3)(img_input)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Conv2D(64, 4, strides=2)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Conv2D(64, 4, strides=2)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Conv2D(64, 4, strides=2)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        # 重要的技巧（新增一個dropout層）\n",
    "        x = Dropout(0.4)(x)\n",
    "\n",
    "        # 分類層\n",
    "        validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        discriminator = Model(img_input, validity)\n",
    "#         discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def connect(self):\n",
    "        self.generator = self.build_generator()\n",
    "        print(self.generator.count_params())\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        print(self.discriminator.count_params())\n",
    "        self.optimizer = Adam(.0002, .5)\n",
    "        # Optimizer用Adam, Learning rate=0.0001~0.0002, 切勿調高\n",
    "        self.discriminator.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "        \n",
    "        noise = Input(shape=(self.noise_dim,))\n",
    "        img = self.generator(noise)\n",
    "        self.discriminator.trainable = False # 在訓練G時, 鎖定D\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(noise, validity)\n",
    "        self.combined.compile(optimizer=self.optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=200):\n",
    "        self.history = []\n",
    "        valid = np.ones((batch_size, 1)) # 1 = 真實圖片\n",
    "        fake = np.zeros((batch_size, 1)) # 0 = 生成圖片\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for i, real_img in enumerate(self.dataloader.sampling_data(batch_size)):\n",
    "                # Train D\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                fake_img = self.generator.predict(noise)\n",
    "\n",
    "                d_loss_real, real_acc = self.discriminator.train_on_batch(real_img, valid[:len(real_img)])\n",
    "                d_loss_fake, fake_acc = self.discriminator.train_on_batch(fake_img, fake)\n",
    "                d_loss = .5 * (d_loss_real + d_loss_fake)\n",
    "                d_acc = .5 * (real_acc + fake_acc)\n",
    "                                                                          \n",
    "                # Train G\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                if i % sample_interval == 0:\n",
    "                    info = {\n",
    "                            'epoch': e,\n",
    "                            'iter': i,\n",
    "                            'd_loss': d_loss,\n",
    "                            'd_acc': d_acc*100,\n",
    "                            'g_loss': g_loss\n",
    "                            }\n",
    "                    self.history.append(list(info.values()))\n",
    "                    print('[Epoch %(epoch)d][Iteration %(iter)d][D loss: %(d_loss).6f, acc: %(d_acc).2f%%][G loss: %(g_loss).6f]' % info)\n",
    "            self.__sample_image(e)\n",
    "        return self.history\n",
    "\n",
    "    def __sample_image(self, epoch):\n",
    "        r, c = 8, 8 # 列, 欄\n",
    "        noise = np.random.standard_normal((r*c, self.noise_dim))\n",
    "        img = self.generator.predict(noise).reshape((r, c) + self.img_size)\n",
    "        img = img * .5 + .5\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        axs = fig.subplots(r, c)\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(img[i, j])\n",
    "                axs[i, j].axis('off')\n",
    "        fig.savefig('./Image/%5d.png' % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 12:02:01.265065 16612 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16054019\n",
      "795009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][Iteration 0][D loss: 0.686763, acc: 33.59%][G loss: 0.565237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][Iteration 10][D loss: 0.332571, acc: 89.06%][G loss: 0.693477]\n",
      "[Epoch 0][Iteration 20][D loss: 0.011243, acc: 100.00%][G loss: 0.018931]\n",
      "[Epoch 0][Iteration 30][D loss: 0.008728, acc: 100.00%][G loss: 0.003518]\n",
      "[Epoch 0][Iteration 40][D loss: 2.025790, acc: 50.00%][G loss: 1.915373]\n",
      "[Epoch 0][Iteration 50][D loss: 0.016972, acc: 100.00%][G loss: 0.010949]\n",
      "[Epoch 0][Iteration 60][D loss: 0.010831, acc: 100.00%][G loss: 0.014258]\n",
      "[Epoch 0][Iteration 70][D loss: 0.204513, acc: 92.19%][G loss: 2.186549]\n",
      "[Epoch 0][Iteration 80][D loss: 0.039417, acc: 98.44%][G loss: 2.422491]\n",
      "[Epoch 0][Iteration 90][D loss: 0.071767, acc: 100.00%][G loss: 3.939535]\n",
      "[Epoch 0][Iteration 100][D loss: 0.015481, acc: 100.00%][G loss: 4.598910]\n",
      "[Epoch 0][Iteration 110][D loss: 0.452973, acc: 72.66%][G loss: 3.645448]\n",
      "[Epoch 0][Iteration 120][D loss: 0.034076, acc: 100.00%][G loss: 3.584036]\n",
      "[Epoch 0][Iteration 130][D loss: 0.123536, acc: 96.88%][G loss: 5.478034]\n",
      "[Epoch 0][Iteration 140][D loss: 0.089070, acc: 99.22%][G loss: 3.258919]\n",
      "[Epoch 0][Iteration 150][D loss: 0.222017, acc: 90.62%][G loss: 5.035730]\n",
      "[Epoch 1][Iteration 0][D loss: 0.055766, acc: 100.00%][G loss: 3.691411]\n",
      "[Epoch 1][Iteration 10][D loss: 0.410676, acc: 80.47%][G loss: 3.162284]\n",
      "[Epoch 1][Iteration 20][D loss: 0.205256, acc: 91.41%][G loss: 2.610899]\n",
      "[Epoch 1][Iteration 30][D loss: 0.087624, acc: 97.66%][G loss: 0.230843]\n",
      "[Epoch 1][Iteration 40][D loss: 0.108801, acc: 96.88%][G loss: 0.110003]\n",
      "[Epoch 1][Iteration 50][D loss: 0.214853, acc: 93.75%][G loss: 0.315250]\n",
      "[Epoch 1][Iteration 60][D loss: 0.352541, acc: 89.06%][G loss: 0.218290]\n",
      "[Epoch 1][Iteration 70][D loss: 0.130139, acc: 100.00%][G loss: 0.195774]\n",
      "[Epoch 1][Iteration 80][D loss: 0.685180, acc: 52.34%][G loss: 1.247572]\n",
      "[Epoch 1][Iteration 90][D loss: 0.704199, acc: 52.34%][G loss: 0.284613]\n",
      "[Epoch 1][Iteration 100][D loss: 0.414717, acc: 95.31%][G loss: 0.174883]\n",
      "[Epoch 1][Iteration 110][D loss: 0.177332, acc: 99.22%][G loss: 0.045363]\n",
      "[Epoch 1][Iteration 120][D loss: 0.089301, acc: 98.44%][G loss: 0.017627]\n",
      "[Epoch 1][Iteration 130][D loss: 0.035605, acc: 100.00%][G loss: 0.015639]\n",
      "[Epoch 1][Iteration 140][D loss: 0.034493, acc: 99.22%][G loss: 0.014862]\n",
      "[Epoch 1][Iteration 150][D loss: 0.025012, acc: 100.00%][G loss: 0.017666]\n",
      "[Epoch 2][Iteration 0][D loss: 0.043654, acc: 99.22%][G loss: 0.009087]\n",
      "[Epoch 2][Iteration 10][D loss: 0.056299, acc: 99.22%][G loss: 0.011501]\n",
      "[Epoch 2][Iteration 20][D loss: 0.038756, acc: 100.00%][G loss: 0.018378]\n",
      "[Epoch 2][Iteration 30][D loss: 0.586927, acc: 60.16%][G loss: 2.206672]\n",
      "[Epoch 2][Iteration 40][D loss: 1.660901, acc: 35.16%][G loss: 1.816401]\n",
      "[Epoch 2][Iteration 50][D loss: 0.526398, acc: 75.00%][G loss: 1.300249]\n",
      "[Epoch 2][Iteration 60][D loss: 0.408238, acc: 91.41%][G loss: 0.761953]\n",
      "[Epoch 2][Iteration 70][D loss: 0.338468, acc: 97.66%][G loss: 0.955068]\n",
      "[Epoch 2][Iteration 80][D loss: 0.325573, acc: 93.75%][G loss: 1.648210]\n",
      "[Epoch 2][Iteration 90][D loss: 0.856818, acc: 37.50%][G loss: 1.394460]\n",
      "[Epoch 2][Iteration 100][D loss: 0.509087, acc: 82.03%][G loss: 1.777464]\n",
      "[Epoch 2][Iteration 110][D loss: 0.030774, acc: 100.00%][G loss: 0.000986]\n",
      "[Epoch 2][Iteration 120][D loss: 0.010484, acc: 100.00%][G loss: 0.001910]\n",
      "[Epoch 2][Iteration 130][D loss: 0.021319, acc: 99.22%][G loss: 0.006186]\n",
      "[Epoch 2][Iteration 140][D loss: 0.010775, acc: 100.00%][G loss: 0.041482]\n",
      "[Epoch 2][Iteration 150][D loss: 1.893577, acc: 50.00%][G loss: 6.328155]\n",
      "[Epoch 3][Iteration 0][D loss: 0.165716, acc: 98.44%][G loss: 2.537455]\n",
      "[Epoch 3][Iteration 10][D loss: 0.640359, acc: 71.88%][G loss: 1.346749]\n",
      "[Epoch 3][Iteration 20][D loss: 0.622531, acc: 66.41%][G loss: 1.295127]\n",
      "[Epoch 3][Iteration 30][D loss: 0.466471, acc: 83.59%][G loss: 1.971351]\n",
      "[Epoch 3][Iteration 40][D loss: 0.577566, acc: 68.75%][G loss: 1.344056]\n",
      "[Epoch 3][Iteration 50][D loss: 0.285683, acc: 94.53%][G loss: 0.770889]\n",
      "[Epoch 3][Iteration 60][D loss: 0.186726, acc: 96.09%][G loss: 1.990948]\n",
      "[Epoch 3][Iteration 70][D loss: 0.475506, acc: 82.03%][G loss: 3.526430]\n",
      "[Epoch 3][Iteration 80][D loss: 0.650688, acc: 66.41%][G loss: 1.889132]\n",
      "[Epoch 3][Iteration 90][D loss: 0.300008, acc: 90.62%][G loss: 1.690878]\n",
      "[Epoch 3][Iteration 100][D loss: 0.249677, acc: 93.75%][G loss: 0.883111]\n",
      "[Epoch 3][Iteration 110][D loss: 0.054421, acc: 100.00%][G loss: 1.222315]\n",
      "[Epoch 3][Iteration 120][D loss: 0.130724, acc: 96.09%][G loss: 1.250137]\n",
      "[Epoch 3][Iteration 130][D loss: 0.457821, acc: 85.16%][G loss: 2.382547]\n",
      "[Epoch 3][Iteration 140][D loss: 0.109060, acc: 96.88%][G loss: 1.942500]\n",
      "[Epoch 3][Iteration 150][D loss: 0.182219, acc: 95.31%][G loss: 2.543616]\n",
      "[Epoch 4][Iteration 0][D loss: 0.289420, acc: 89.06%][G loss: 4.942388]\n",
      "[Epoch 4][Iteration 10][D loss: 0.185561, acc: 94.53%][G loss: 0.719190]\n",
      "[Epoch 4][Iteration 20][D loss: 0.033811, acc: 100.00%][G loss: 0.430971]\n",
      "[Epoch 4][Iteration 30][D loss: 0.625779, acc: 75.00%][G loss: 7.775628]\n",
      "[Epoch 4][Iteration 40][D loss: 0.028869, acc: 99.22%][G loss: 0.067241]\n",
      "[Epoch 4][Iteration 50][D loss: 0.013578, acc: 100.00%][G loss: 0.064061]\n",
      "[Epoch 4][Iteration 60][D loss: 0.012781, acc: 100.00%][G loss: 0.163229]\n",
      "[Epoch 4][Iteration 70][D loss: 0.021246, acc: 99.22%][G loss: 0.360151]\n",
      "[Epoch 4][Iteration 80][D loss: 0.187180, acc: 96.09%][G loss: 1.415545]\n",
      "[Epoch 4][Iteration 90][D loss: 0.062402, acc: 100.00%][G loss: 2.776496]\n",
      "[Epoch 4][Iteration 100][D loss: 0.159643, acc: 96.09%][G loss: 2.601069]\n",
      "[Epoch 4][Iteration 110][D loss: 0.194294, acc: 97.66%][G loss: 0.428621]\n",
      "[Epoch 4][Iteration 120][D loss: 0.536165, acc: 65.62%][G loss: 7.567354]\n",
      "[Epoch 4][Iteration 130][D loss: 0.155221, acc: 96.09%][G loss: 5.285899]\n",
      "[Epoch 4][Iteration 140][D loss: 0.330868, acc: 90.62%][G loss: 0.942166]\n",
      "[Epoch 4][Iteration 150][D loss: 0.242762, acc: 91.41%][G loss: 3.279771]\n",
      "[Epoch 5][Iteration 0][D loss: 0.421662, acc: 85.16%][G loss: 3.169161]\n",
      "[Epoch 5][Iteration 10][D loss: 0.183921, acc: 93.75%][G loss: 0.429089]\n",
      "[Epoch 5][Iteration 20][D loss: 0.049189, acc: 99.22%][G loss: 0.136102]\n",
      "[Epoch 5][Iteration 30][D loss: 0.031842, acc: 100.00%][G loss: 0.075533]\n",
      "[Epoch 5][Iteration 40][D loss: 0.017957, acc: 100.00%][G loss: 0.038633]\n",
      "[Epoch 5][Iteration 50][D loss: 0.020753, acc: 100.00%][G loss: 0.033649]\n",
      "[Epoch 5][Iteration 60][D loss: 0.014690, acc: 100.00%][G loss: 0.048845]\n",
      "[Epoch 5][Iteration 70][D loss: 0.020413, acc: 100.00%][G loss: 0.092036]\n",
      "[Epoch 5][Iteration 80][D loss: 0.022517, acc: 100.00%][G loss: 0.083289]\n",
      "[Epoch 5][Iteration 90][D loss: 0.023848, acc: 99.22%][G loss: 0.105394]\n",
      "[Epoch 5][Iteration 100][D loss: 0.016668, acc: 100.00%][G loss: 0.123572]\n",
      "[Epoch 5][Iteration 110][D loss: 0.127488, acc: 96.09%][G loss: 6.368587]\n",
      "[Epoch 5][Iteration 120][D loss: 0.198125, acc: 97.66%][G loss: 2.234234]\n",
      "[Epoch 5][Iteration 130][D loss: 0.334644, acc: 89.06%][G loss: 6.679129]\n",
      "[Epoch 5][Iteration 140][D loss: 0.039330, acc: 100.00%][G loss: 0.624575]\n",
      "[Epoch 5][Iteration 150][D loss: 0.086144, acc: 99.22%][G loss: 0.963504]\n",
      "[Epoch 6][Iteration 0][D loss: 0.301663, acc: 91.41%][G loss: 1.459367]\n",
      "[Epoch 6][Iteration 10][D loss: 0.262484, acc: 90.62%][G loss: 1.775519]\n",
      "[Epoch 6][Iteration 20][D loss: 0.280756, acc: 95.31%][G loss: 2.661117]\n",
      "[Epoch 6][Iteration 30][D loss: 0.499099, acc: 69.53%][G loss: 1.151618]\n",
      "[Epoch 6][Iteration 40][D loss: 0.042103, acc: 99.22%][G loss: 0.068136]\n",
      "[Epoch 6][Iteration 50][D loss: 0.021490, acc: 100.00%][G loss: 0.118846]\n",
      "[Epoch 6][Iteration 60][D loss: 0.015461, acc: 100.00%][G loss: 0.108485]\n",
      "[Epoch 6][Iteration 70][D loss: 0.017695, acc: 100.00%][G loss: 0.077992]\n",
      "[Epoch 6][Iteration 80][D loss: 0.011087, acc: 100.00%][G loss: 0.041611]\n",
      "[Epoch 6][Iteration 90][D loss: 0.009724, acc: 100.00%][G loss: 0.043713]\n",
      "[Epoch 6][Iteration 100][D loss: 0.008660, acc: 100.00%][G loss: 0.059651]\n",
      "[Epoch 6][Iteration 110][D loss: 0.008420, acc: 100.00%][G loss: 0.031012]\n",
      "[Epoch 6][Iteration 120][D loss: 0.007175, acc: 100.00%][G loss: 0.018167]\n",
      "[Epoch 6][Iteration 130][D loss: 0.005221, acc: 100.00%][G loss: 0.009715]\n",
      "[Epoch 6][Iteration 140][D loss: 0.008620, acc: 100.00%][G loss: 0.010417]\n",
      "[Epoch 6][Iteration 150][D loss: 0.015154, acc: 99.22%][G loss: 0.009549]\n",
      "[Epoch 7][Iteration 0][D loss: 0.006793, acc: 100.00%][G loss: 0.026890]\n",
      "[Epoch 7][Iteration 10][D loss: 0.003052, acc: 100.00%][G loss: 0.012977]\n",
      "[Epoch 7][Iteration 20][D loss: 0.005561, acc: 100.00%][G loss: 0.008668]\n",
      "[Epoch 7][Iteration 30][D loss: 0.003092, acc: 100.00%][G loss: 0.008724]\n",
      "[Epoch 7][Iteration 40][D loss: 0.004230, acc: 100.00%][G loss: 0.007871]\n",
      "[Epoch 7][Iteration 50][D loss: 0.014567, acc: 100.00%][G loss: 0.006150]\n",
      "[Epoch 7][Iteration 60][D loss: 0.003694, acc: 100.00%][G loss: 0.006141]\n",
      "[Epoch 7][Iteration 70][D loss: 0.003327, acc: 100.00%][G loss: 0.003447]\n",
      "[Epoch 7][Iteration 80][D loss: 0.004741, acc: 100.00%][G loss: 0.004436]\n",
      "[Epoch 7][Iteration 90][D loss: 0.001762, acc: 100.00%][G loss: 0.005737]\n",
      "[Epoch 7][Iteration 100][D loss: 0.009506, acc: 100.00%][G loss: 0.004412]\n",
      "[Epoch 7][Iteration 110][D loss: 0.004102, acc: 100.00%][G loss: 0.008304]\n",
      "[Epoch 7][Iteration 120][D loss: 0.009732, acc: 100.00%][G loss: 0.013450]\n",
      "[Epoch 7][Iteration 130][D loss: 0.011521, acc: 100.00%][G loss: 0.011862]\n",
      "[Epoch 7][Iteration 140][D loss: 0.031327, acc: 99.22%][G loss: 0.063960]\n",
      "[Epoch 7][Iteration 150][D loss: 0.160701, acc: 98.44%][G loss: 1.415620]\n",
      "[Epoch 8][Iteration 0][D loss: 0.055628, acc: 99.22%][G loss: 1.257986]\n",
      "[Epoch 8][Iteration 10][D loss: 0.024696, acc: 100.00%][G loss: 0.375499]\n",
      "[Epoch 8][Iteration 20][D loss: 0.029059, acc: 100.00%][G loss: 0.119262]\n",
      "[Epoch 8][Iteration 30][D loss: 0.056138, acc: 98.44%][G loss: 0.094191]\n",
      "[Epoch 8][Iteration 40][D loss: 0.073739, acc: 99.22%][G loss: 0.231642]\n",
      "[Epoch 8][Iteration 50][D loss: 0.081190, acc: 100.00%][G loss: 0.610592]\n",
      "[Epoch 8][Iteration 60][D loss: 0.189830, acc: 95.31%][G loss: 0.531734]\n",
      "[Epoch 8][Iteration 70][D loss: 0.101564, acc: 96.88%][G loss: 0.388325]\n",
      "[Epoch 8][Iteration 80][D loss: 0.246407, acc: 87.50%][G loss: 15.902895]\n",
      "[Epoch 8][Iteration 90][D loss: 0.048343, acc: 100.00%][G loss: 0.308214]\n",
      "[Epoch 8][Iteration 100][D loss: 0.604480, acc: 66.41%][G loss: 0.399468]\n",
      "[Epoch 8][Iteration 110][D loss: 0.010038, acc: 100.00%][G loss: 0.004796]\n",
      "[Epoch 8][Iteration 120][D loss: 0.006683, acc: 100.00%][G loss: 0.016470]\n",
      "[Epoch 8][Iteration 130][D loss: 0.010363, acc: 100.00%][G loss: 0.018494]\n",
      "[Epoch 8][Iteration 140][D loss: 0.019149, acc: 100.00%][G loss: 0.033333]\n",
      "[Epoch 8][Iteration 150][D loss: 0.029855, acc: 99.22%][G loss: 0.081588]\n",
      "[Epoch 9][Iteration 0][D loss: 0.698006, acc: 64.06%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 10][D loss: 0.000237, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 20][D loss: 0.000262, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 30][D loss: 0.000124, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 40][D loss: 0.000289, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 50][D loss: 0.000201, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 60][D loss: 0.000129, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 70][D loss: 0.000108, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 80][D loss: 0.000240, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 90][D loss: 0.000090, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 100][D loss: 0.000090, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 110][D loss: 0.000084, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 120][D loss: 0.000113, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 130][D loss: 0.000089, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 140][D loss: 0.000188, acc: 100.00%][G loss: 16.118095]\n",
      "[Epoch 9][Iteration 150][D loss: 0.000081, acc: 100.00%][G loss: 16.118095]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.6867626905441284, 33.59375, 0.5652367],\n",
       " [0, 10, 0.3325706422328949, 89.0625, 0.6934769],\n",
       " [0, 20, 0.011243059299886227, 100.0, 0.018930769],\n",
       " [0, 30, 0.008727616630494595, 100.0, 0.0035178675],\n",
       " [0, 40, 2.0257904529571533, 50.0, 1.9153728],\n",
       " [0, 50, 0.016972023993730545, 100.0, 0.010948712],\n",
       " [0, 60, 0.01083073765039444, 100.0, 0.014258493],\n",
       " [0, 70, 0.2045128345489502, 92.1875, 2.1865487],\n",
       " [0, 80, 0.03941744938492775, 98.4375, 2.4224913],\n",
       " [0, 90, 0.07176686823368073, 100.0, 3.9395347],\n",
       " [0, 100, 0.015480735339224339, 100.0, 4.5989103],\n",
       " [0, 110, 0.4529734253883362, 72.65625, 3.6454482],\n",
       " [0, 120, 0.034075964242219925, 100.0, 3.5840364],\n",
       " [0, 130, 0.12353583425283432, 96.875, 5.4780335],\n",
       " [0, 140, 0.08906975388526917, 99.21875, 3.258919],\n",
       " [0, 150, 0.22201690077781677, 90.625, 5.0357304],\n",
       " [1, 0, 0.05576566979289055, 100.0, 3.691411],\n",
       " [1, 10, 0.4106762409210205, 80.46875, 3.1622844],\n",
       " [1, 20, 0.2052556872367859, 91.40625, 2.610899],\n",
       " [1, 30, 0.08762390911579132, 97.65625, 0.23084342],\n",
       " [1, 40, 0.10880143195390701, 96.875, 0.11000259],\n",
       " [1, 50, 0.21485286951065063, 93.75, 0.3152504],\n",
       " [1, 60, 0.3525407314300537, 89.0625, 0.21829018],\n",
       " [1, 70, 0.13013865053653717, 100.0, 0.1957737],\n",
       " [1, 80, 0.6851804852485657, 52.34375, 1.2475724],\n",
       " [1, 90, 0.7041993141174316, 52.34375, 0.28461283],\n",
       " [1, 100, 0.41471701860427856, 95.3125, 0.17488343],\n",
       " [1, 110, 0.17733177542686462, 99.21875, 0.045362942],\n",
       " [1, 120, 0.0893007144331932, 98.4375, 0.01762741],\n",
       " [1, 130, 0.035605065524578094, 100.0, 0.015639327],\n",
       " [1, 140, 0.03449345380067825, 99.21875, 0.014861847],\n",
       " [1, 150, 0.025011979043483734, 100.0, 0.01766604],\n",
       " [2, 0, 0.04365446791052818, 99.21875, 0.0090867365],\n",
       " [2, 10, 0.05629870295524597, 99.21875, 0.011500925],\n",
       " [2, 20, 0.038756050169467926, 100.0, 0.018378206],\n",
       " [2, 30, 0.586927056312561, 60.15625, 2.206672],\n",
       " [2, 40, 1.6609008312225342, 35.15625, 1.8164006],\n",
       " [2, 50, 0.5263984203338623, 75.0, 1.3002486],\n",
       " [2, 60, 0.4082384705543518, 91.40625, 0.76195335],\n",
       " [2, 70, 0.3384684920310974, 97.65625, 0.95506775],\n",
       " [2, 80, 0.3255731463432312, 93.75, 1.6482097],\n",
       " [2, 90, 0.856817901134491, 37.5, 1.3944604],\n",
       " [2, 100, 0.509086549282074, 82.03125, 1.7774639],\n",
       " [2, 110, 0.030773933976888657, 100.0, 0.000986228],\n",
       " [2, 120, 0.01048426516354084, 100.0, 0.0019104736],\n",
       " [2, 130, 0.021319495514035225, 99.21875, 0.0061856564],\n",
       " [2, 140, 0.01077486202120781, 100.0, 0.041482475],\n",
       " [2, 150, 1.8935766220092773, 50.0, 6.3281546],\n",
       " [3, 0, 0.16571567952632904, 98.4375, 2.5374553],\n",
       " [3, 10, 0.6403589248657227, 71.875, 1.3467491],\n",
       " [3, 20, 0.6225314736366272, 66.40625, 1.2951274],\n",
       " [3, 30, 0.4664706885814667, 83.59375, 1.9713508],\n",
       " [3, 40, 0.5775659084320068, 68.75, 1.3440558],\n",
       " [3, 50, 0.28568339347839355, 94.53125, 0.7708887],\n",
       " [3, 60, 0.18672606348991394, 96.09375, 1.9909481],\n",
       " [3, 70, 0.4755055010318756, 82.03125, 3.5264301],\n",
       " [3, 80, 0.6506880521774292, 66.40625, 1.8891318],\n",
       " [3, 90, 0.300007700920105, 90.625, 1.6908784],\n",
       " [3, 100, 0.24967655539512634, 93.75, 0.8831108],\n",
       " [3, 110, 0.05442114919424057, 100.0, 1.2223151],\n",
       " [3, 120, 0.1307244598865509, 96.09375, 1.2501366],\n",
       " [3, 130, 0.4578210413455963, 85.15625, 2.382547],\n",
       " [3, 140, 0.10905973613262177, 96.875, 1.9425004],\n",
       " [3, 150, 0.1822187602519989, 95.3125, 2.5436156],\n",
       " [4, 0, 0.289420485496521, 89.0625, 4.9423876],\n",
       " [4, 10, 0.18556129932403564, 94.53125, 0.71918964],\n",
       " [4, 20, 0.03381098806858063, 100.0, 0.43097112],\n",
       " [4, 30, 0.6257787942886353, 75.0, 7.7756276],\n",
       " [4, 40, 0.0288693867623806, 99.21875, 0.06724102],\n",
       " [4, 50, 0.013577994890511036, 100.0, 0.06406112],\n",
       " [4, 60, 0.012781447730958462, 100.0, 0.16322908],\n",
       " [4, 70, 0.021245554089546204, 99.21875, 0.3601508],\n",
       " [4, 80, 0.18717963993549347, 96.09375, 1.4155452],\n",
       " [4, 90, 0.062401965260505676, 100.0, 2.7764964],\n",
       " [4, 100, 0.1596428006887436, 96.09375, 2.601069],\n",
       " [4, 110, 0.19429394602775574, 97.65625, 0.42862117],\n",
       " [4, 120, 0.5361654758453369, 65.625, 7.567354],\n",
       " [4, 130, 0.1552211046218872, 96.09375, 5.285899],\n",
       " [4, 140, 0.33086836338043213, 90.625, 0.94216627],\n",
       " [4, 150, 0.24276188015937805, 91.40625, 3.2797713],\n",
       " [5, 0, 0.4216620922088623, 85.15625, 3.1691613],\n",
       " [5, 10, 0.18392065167427063, 93.75, 0.42908883],\n",
       " [5, 20, 0.049188993871212006, 99.21875, 0.1361025],\n",
       " [5, 30, 0.031841978430747986, 100.0, 0.075533226],\n",
       " [5, 40, 0.017956562340259552, 100.0, 0.038633246],\n",
       " [5, 50, 0.020752908661961555, 100.0, 0.033648603],\n",
       " [5, 60, 0.01469033770263195, 100.0, 0.048844762],\n",
       " [5, 70, 0.020412681624293327, 100.0, 0.09203594],\n",
       " [5, 80, 0.022517450153827667, 100.0, 0.083289176],\n",
       " [5, 90, 0.023848388344049454, 99.21875, 0.10539386],\n",
       " [5, 100, 0.016667868942022324, 100.0, 0.123572275],\n",
       " [5, 110, 0.12748754024505615, 96.09375, 6.3685875],\n",
       " [5, 120, 0.19812484085559845, 97.65625, 2.2342339],\n",
       " [5, 130, 0.33464378118515015, 89.0625, 6.679129],\n",
       " [5, 140, 0.039329901337623596, 100.0, 0.62457454],\n",
       " [5, 150, 0.08614428341388702, 99.21875, 0.9635042],\n",
       " [6, 0, 0.3016626238822937, 91.40625, 1.4593673],\n",
       " [6, 10, 0.26248443126678467, 90.625, 1.7755195],\n",
       " [6, 20, 0.28075605630874634, 95.3125, 2.6611166],\n",
       " [6, 30, 0.49909916520118713, 69.53125, 1.1516175],\n",
       " [6, 40, 0.04210319370031357, 99.21875, 0.068136185],\n",
       " [6, 50, 0.02148950845003128, 100.0, 0.118845835],\n",
       " [6, 60, 0.015460608527064323, 100.0, 0.108484626],\n",
       " [6, 70, 0.01769542694091797, 100.0, 0.07799167],\n",
       " [6, 80, 0.011087190359830856, 100.0, 0.041610774],\n",
       " [6, 90, 0.009723816066980362, 100.0, 0.043712854],\n",
       " [6, 100, 0.008660176768898964, 100.0, 0.05965108],\n",
       " [6, 110, 0.008420143276453018, 100.0, 0.03101224],\n",
       " [6, 120, 0.00717479083687067, 100.0, 0.018167203],\n",
       " [6, 130, 0.005220549181103706, 100.0, 0.009714717],\n",
       " [6, 140, 0.00861998088657856, 100.0, 0.010416778],\n",
       " [6, 150, 0.015153564512729645, 99.21875, 0.00954877],\n",
       " [7, 0, 0.00679328478872776, 100.0, 0.026889969],\n",
       " [7, 10, 0.0030516409315168858, 100.0, 0.012976892],\n",
       " [7, 20, 0.005561132915318012, 100.0, 0.008668289],\n",
       " [7, 30, 0.0030915243551135063, 100.0, 0.00872355],\n",
       " [7, 40, 0.004230363294482231, 100.0, 0.007871444],\n",
       " [7, 50, 0.01456691324710846, 100.0, 0.0061496645],\n",
       " [7, 60, 0.0036936774849891663, 100.0, 0.0061409557],\n",
       " [7, 70, 0.0033272793516516685, 100.0, 0.00344693],\n",
       " [7, 80, 0.0047405678778886795, 100.0, 0.0044355392],\n",
       " [7, 90, 0.0017620171420276165, 100.0, 0.0057370844],\n",
       " [7, 100, 0.009505854919552803, 100.0, 0.004411574],\n",
       " [7, 110, 0.004102243576198816, 100.0, 0.00830406],\n",
       " [7, 120, 0.009732197970151901, 100.0, 0.013450367],\n",
       " [7, 130, 0.01152106188237667, 100.0, 0.011862143],\n",
       " [7, 140, 0.031326957046985626, 99.21875, 0.06396002],\n",
       " [7, 150, 0.16070060431957245, 98.4375, 1.4156196],\n",
       " [8, 0, 0.05562788248062134, 99.21875, 1.2579863],\n",
       " [8, 10, 0.0246955007314682, 100.0, 0.37549886],\n",
       " [8, 20, 0.029059164226055145, 100.0, 0.119261995],\n",
       " [8, 30, 0.05613849312067032, 98.4375, 0.09419075],\n",
       " [8, 40, 0.07373949885368347, 99.21875, 0.23164241],\n",
       " [8, 50, 0.08119029551744461, 100.0, 0.61059237],\n",
       " [8, 60, 0.18983006477355957, 95.3125, 0.53173363],\n",
       " [8, 70, 0.10156391561031342, 96.875, 0.38832524],\n",
       " [8, 80, 0.24640673398971558, 87.5, 15.902895],\n",
       " [8, 90, 0.048342835158109665, 100.0, 0.3082139],\n",
       " [8, 100, 0.6044796705245972, 66.40625, 0.39946836],\n",
       " [8, 110, 0.010037964209914207, 100.0, 0.0047959574],\n",
       " [8, 120, 0.006682948209345341, 100.0, 0.016470231],\n",
       " [8, 130, 0.010363025590777397, 100.0, 0.01849382],\n",
       " [8, 140, 0.019148845225572586, 100.0, 0.033333164],\n",
       " [8, 150, 0.02985537424683571, 99.21875, 0.08158767],\n",
       " [9, 0, 0.698006272315979, 64.0625, 16.118095],\n",
       " [9, 10, 0.00023731254623271525, 100.0, 16.118095],\n",
       " [9, 20, 0.00026156764943152666, 100.0, 16.118095],\n",
       " [9, 30, 0.00012379165855236351, 100.0, 16.118095],\n",
       " [9, 40, 0.0002893824130296707, 100.0, 16.118095],\n",
       " [9, 50, 0.00020099498215131462, 100.0, 16.118095],\n",
       " [9, 60, 0.00012892112135887146, 100.0, 16.118095],\n",
       " [9, 70, 0.00010817249858519062, 100.0, 16.118095],\n",
       " [9, 80, 0.0002403965627308935, 100.0, 16.118095],\n",
       " [9, 90, 8.954139775596559e-05, 100.0, 16.118095],\n",
       " [9, 100, 8.961635467130691e-05, 100.0, 16.118095],\n",
       " [9, 110, 8.364109817193821e-05, 100.0, 16.118095],\n",
       " [9, 120, 0.0001128912263084203, 100.0, 16.118095],\n",
       " [9, 130, 8.863663242664188e-05, 100.0, 16.118095],\n",
       " [9, 140, 0.00018844666192308068, 100.0, 16.118095],\n",
       " [9, 150, 8.149839413817972e-05, 100.0, 16.118095]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gan = GAN(128, img_size=(64, 64, 3))\n",
    "gan.connect()\n",
    "gan.train(10, 64, sample_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
