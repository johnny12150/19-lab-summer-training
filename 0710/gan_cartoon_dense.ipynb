{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用昨天的卡通資料集來練GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, UpSampling2D, Dense, Flatten, Input, BatchNormalization, Reshape, LeakyReLU, Conv2DTranspose, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, folder_path, img_size):\n",
    "        self.folder_path = folder_path\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.path_list = glob(folder_path) # 讀取資料夾全部圖片路徑\n",
    "        assert len(self.path_list) > 0, 'path not existed!'\n",
    "    \n",
    "    def __imread(self, img_path):\n",
    "        '''讀取圖片'''\n",
    "        return np.array(Image.open(img_path).convert('RGB').resize(self.img_size[:-1], Image.ANTIALIAS))\n",
    "    \n",
    "    def sampling_data(self, batch_size, shuffle=True):\n",
    "        img_path_list = self.path_list\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(img_path_list)\n",
    "            \n",
    "        for batch_idx in range(0, len(img_path_list), batch_size):\n",
    "            path_set = img_path_list[batch_idx : batch_idx + batch_size]\n",
    "            \n",
    "            # 預設空間，避免 append很慢\n",
    "            img_set = np.zeros((len(path_set),) + self.img_size)\n",
    "            for img_idx, path in enumerate(path_set):\n",
    "                img_set[img_idx] = self.__imread(path)\n",
    "            \n",
    "            # 127.5是255的一半，一到負一之間\n",
    "            img_set = img_set / 127.5 - 1\n",
    "            # 暫停\n",
    "            yield img_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, noise_dim, img_size=(64, 64, 3)):\n",
    "        self.noise_dim = noise_dim # noise_dim = 雜訊維度\n",
    "        self.img_size = img_size # img_size = 圖片大小\n",
    "        self.dataloader = DataLoader('../0709/Preview/cartoon/*.png', self.img_size)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise_input = Input(shape=(self.noise_dim,))\n",
    "        \n",
    "        x = Dense(128)(noise_input)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        \n",
    "        x = Dense(128)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        \n",
    "        x = Dense(64)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        \n",
    "        x = Dense(64*64*3, activation='tanh')(x)\n",
    "        \n",
    "        img = Reshape((64, 64, 3))(x)\n",
    "\n",
    "        generator = Model(noise_input, img)\n",
    "#         generator.summary()\n",
    "        return generator\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_input = Input(shape=self.img_size)\n",
    "        # 把照片拉直好放入dense\n",
    "        x = Flatten()(img_input)\n",
    "        \n",
    "        x = Dense(32)(x)\n",
    "        # 加了BatchNormalization會導致訓練失敗\n",
    "#         x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        x = Dense(32)(x)\n",
    "#         x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        # 重要的技巧（新增一個dropout層）\n",
    "#         x = Dropout(0.4)(x)\n",
    "\n",
    "        # 分類層\n",
    "        validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        discriminator = Model(img_input, validity)\n",
    "#         discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def connect(self):\n",
    "        self.generator = self.build_generator()\n",
    "        print(self.generator.count_params())\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        print(self.discriminator.count_params())\n",
    "        self.optimizer = Adam(.0002, .5)\n",
    "        # Optimizer用Adam, Learning rate=0.0001~0.0002, 切勿調高\n",
    "        self.discriminator.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "        \n",
    "        noise = Input(shape=(self.noise_dim,))\n",
    "        img = self.generator(noise)\n",
    "        self.discriminator.trainable = False # 在訓練G時, 鎖定D\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(noise, validity)\n",
    "        self.combined.compile(optimizer=self.optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=200):\n",
    "        self.history = []\n",
    "        valid = np.ones((batch_size, 1)) # 1 = 真實圖片\n",
    "        fake = np.zeros((batch_size, 1)) # 0 = 生成圖片\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for i, real_img in enumerate(self.dataloader.sampling_data(batch_size)):\n",
    "                # Train D\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                fake_img = self.generator.predict(noise)\n",
    "\n",
    "                d_loss_real, real_acc = self.discriminator.train_on_batch(real_img, valid[:len(real_img)])\n",
    "                d_loss_fake, fake_acc = self.discriminator.train_on_batch(fake_img, fake)\n",
    "                d_loss = .5 * (d_loss_real + d_loss_fake)\n",
    "                d_acc = .5 * (real_acc + fake_acc)\n",
    "                                                                          \n",
    "                # Train G\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                if i % sample_interval == 0:\n",
    "                    info = {\n",
    "                            'epoch': e,\n",
    "                            'iter': i,\n",
    "                            'd_loss': d_loss,\n",
    "                            'd_acc': d_acc*100,\n",
    "                            'g_loss': g_loss\n",
    "                            }\n",
    "                    self.history.append(list(info.values()))\n",
    "                    print('[Epoch %(epoch)d][Iteration %(iter)d][D loss: %(d_loss).6f, acc: %(d_acc).2f%%][G loss: %(g_loss).6f]' % info)\n",
    "            self.__sample_image(e)\n",
    "        return self.history\n",
    "\n",
    "    def __sample_image(self, epoch):\n",
    "        r, c = 8, 8 # 列, 欄\n",
    "        noise = np.random.standard_normal((r*c, self.noise_dim))\n",
    "        img = self.generator.predict(noise).reshape((r, c) + self.img_size)\n",
    "        img = img * .5 + .5\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        axs = fig.subplots(r, c)\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(img[i, j])\n",
    "                axs[i, j].axis('off')\n",
    "        fig.savefig('./Image/%5d.png' % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652608\n",
      "394337\n",
      "[Epoch 0][Iteration 0][D loss: 0.581307, acc: 60.94%][G loss: 0.742023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][Iteration 10][D loss: 1.162078, acc: 52.34%][G loss: 0.252068]\n",
      "[Epoch 0][Iteration 20][D loss: 0.722191, acc: 50.00%][G loss: 0.392950]\n",
      "[Epoch 0][Iteration 30][D loss: 0.632635, acc: 64.06%][G loss: 0.691841]\n",
      "[Epoch 0][Iteration 40][D loss: 0.542671, acc: 65.62%][G loss: 0.804728]\n",
      "[Epoch 0][Iteration 50][D loss: 0.497347, acc: 64.84%][G loss: 0.798103]\n",
      "[Epoch 0][Iteration 60][D loss: 0.403466, acc: 74.22%][G loss: 0.866657]\n",
      "[Epoch 0][Iteration 70][D loss: 0.457039, acc: 69.53%][G loss: 0.771101]\n",
      "[Epoch 0][Iteration 80][D loss: 0.695810, acc: 56.25%][G loss: 0.723735]\n",
      "[Epoch 0][Iteration 90][D loss: 0.604644, acc: 57.03%][G loss: 0.699438]\n",
      "[Epoch 0][Iteration 100][D loss: 0.723906, acc: 53.12%][G loss: 0.736585]\n",
      "[Epoch 0][Iteration 110][D loss: 0.696265, acc: 51.56%][G loss: 0.675631]\n",
      "[Epoch 0][Iteration 120][D loss: 0.794337, acc: 50.78%][G loss: 0.670343]\n",
      "[Epoch 0][Iteration 130][D loss: 0.654213, acc: 55.47%][G loss: 0.519264]\n",
      "[Epoch 0][Iteration 140][D loss: 0.669023, acc: 59.38%][G loss: 0.904419]\n",
      "[Epoch 0][Iteration 150][D loss: 0.809997, acc: 55.47%][G loss: 0.667099]\n",
      "[Epoch 1][Iteration 0][D loss: 0.726728, acc: 54.69%][G loss: 1.052036]\n",
      "[Epoch 1][Iteration 10][D loss: 0.767293, acc: 53.12%][G loss: 0.798287]\n",
      "[Epoch 1][Iteration 20][D loss: 0.714244, acc: 60.16%][G loss: 0.707835]\n",
      "[Epoch 1][Iteration 30][D loss: 0.646210, acc: 58.59%][G loss: 0.662095]\n",
      "[Epoch 1][Iteration 40][D loss: 0.559986, acc: 66.41%][G loss: 0.730580]\n",
      "[Epoch 1][Iteration 50][D loss: 0.563534, acc: 65.62%][G loss: 0.726547]\n",
      "[Epoch 1][Iteration 60][D loss: 0.662358, acc: 59.38%][G loss: 0.742242]\n",
      "[Epoch 1][Iteration 70][D loss: 0.534336, acc: 67.19%][G loss: 0.791862]\n",
      "[Epoch 1][Iteration 80][D loss: 0.656135, acc: 56.25%][G loss: 0.674072]\n",
      "[Epoch 1][Iteration 90][D loss: 0.537325, acc: 64.84%][G loss: 0.712939]\n",
      "[Epoch 1][Iteration 100][D loss: 0.698917, acc: 53.12%][G loss: 0.610051]\n",
      "[Epoch 1][Iteration 110][D loss: 0.539238, acc: 61.72%][G loss: 0.838683]\n",
      "[Epoch 1][Iteration 120][D loss: 0.565814, acc: 64.84%][G loss: 0.655960]\n",
      "[Epoch 1][Iteration 130][D loss: 0.703364, acc: 59.38%][G loss: 0.762414]\n",
      "[Epoch 1][Iteration 140][D loss: 0.853499, acc: 50.78%][G loss: 0.623892]\n",
      "[Epoch 1][Iteration 150][D loss: 0.592363, acc: 59.38%][G loss: 0.765081]\n",
      "[Epoch 2][Iteration 0][D loss: 0.635923, acc: 60.16%][G loss: 0.921850]\n",
      "[Epoch 2][Iteration 10][D loss: 0.784040, acc: 52.34%][G loss: 0.840757]\n",
      "[Epoch 2][Iteration 20][D loss: 0.708740, acc: 60.16%][G loss: 0.802646]\n",
      "[Epoch 2][Iteration 30][D loss: 0.721445, acc: 57.81%][G loss: 0.971695]\n",
      "[Epoch 2][Iteration 40][D loss: 0.683814, acc: 61.72%][G loss: 0.797411]\n",
      "[Epoch 2][Iteration 50][D loss: 0.626378, acc: 56.25%][G loss: 1.107512]\n",
      "[Epoch 2][Iteration 60][D loss: 0.588801, acc: 67.19%][G loss: 1.053805]\n",
      "[Epoch 2][Iteration 70][D loss: 0.883094, acc: 42.97%][G loss: 0.577923]\n",
      "[Epoch 2][Iteration 80][D loss: 0.583204, acc: 57.81%][G loss: 0.873358]\n",
      "[Epoch 2][Iteration 90][D loss: 0.520701, acc: 67.19%][G loss: 0.801815]\n",
      "[Epoch 2][Iteration 100][D loss: 0.583395, acc: 58.59%][G loss: 0.718638]\n",
      "[Epoch 2][Iteration 110][D loss: 0.595543, acc: 58.59%][G loss: 0.832343]\n",
      "[Epoch 2][Iteration 120][D loss: 0.749972, acc: 53.12%][G loss: 0.769671]\n",
      "[Epoch 2][Iteration 130][D loss: 0.684160, acc: 55.47%][G loss: 0.603442]\n",
      "[Epoch 2][Iteration 140][D loss: 0.571705, acc: 67.19%][G loss: 1.079134]\n",
      "[Epoch 2][Iteration 150][D loss: 0.638077, acc: 58.59%][G loss: 0.688983]\n",
      "[Epoch 3][Iteration 0][D loss: 0.663455, acc: 60.16%][G loss: 0.708306]\n",
      "[Epoch 3][Iteration 10][D loss: 0.549256, acc: 62.50%][G loss: 1.017601]\n",
      "[Epoch 3][Iteration 20][D loss: 0.602899, acc: 53.91%][G loss: 0.666664]\n",
      "[Epoch 3][Iteration 30][D loss: 0.566713, acc: 59.38%][G loss: 0.768346]\n",
      "[Epoch 3][Iteration 40][D loss: 0.553479, acc: 68.75%][G loss: 0.914222]\n",
      "[Epoch 3][Iteration 50][D loss: 0.438362, acc: 70.31%][G loss: 0.831773]\n",
      "[Epoch 3][Iteration 60][D loss: 0.407416, acc: 75.78%][G loss: 0.983734]\n",
      "[Epoch 3][Iteration 70][D loss: 0.480507, acc: 68.75%][G loss: 0.830910]\n",
      "[Epoch 3][Iteration 80][D loss: 0.460628, acc: 67.97%][G loss: 0.884736]\n",
      "[Epoch 3][Iteration 90][D loss: 0.500677, acc: 69.53%][G loss: 0.835292]\n",
      "[Epoch 3][Iteration 100][D loss: 0.450200, acc: 77.34%][G loss: 0.806993]\n",
      "[Epoch 3][Iteration 110][D loss: 0.512777, acc: 72.66%][G loss: 0.882031]\n",
      "[Epoch 3][Iteration 120][D loss: 0.427951, acc: 76.56%][G loss: 0.898841]\n",
      "[Epoch 3][Iteration 130][D loss: 0.492336, acc: 71.09%][G loss: 0.858038]\n",
      "[Epoch 3][Iteration 140][D loss: 0.423944, acc: 78.12%][G loss: 0.963390]\n",
      "[Epoch 3][Iteration 150][D loss: 0.430403, acc: 77.34%][G loss: 0.926433]\n",
      "[Epoch 4][Iteration 0][D loss: 0.540084, acc: 71.09%][G loss: 0.862117]\n",
      "[Epoch 4][Iteration 10][D loss: 0.501905, acc: 70.31%][G loss: 0.969072]\n",
      "[Epoch 4][Iteration 20][D loss: 0.514700, acc: 67.97%][G loss: 1.080547]\n",
      "[Epoch 4][Iteration 30][D loss: 0.678295, acc: 65.62%][G loss: 1.211220]\n",
      "[Epoch 4][Iteration 40][D loss: 0.713050, acc: 61.72%][G loss: 1.190328]\n",
      "[Epoch 4][Iteration 50][D loss: 0.722817, acc: 60.16%][G loss: 0.846934]\n",
      "[Epoch 4][Iteration 60][D loss: 0.628841, acc: 66.41%][G loss: 0.984989]\n",
      "[Epoch 4][Iteration 70][D loss: 0.705341, acc: 70.31%][G loss: 0.999215]\n",
      "[Epoch 4][Iteration 80][D loss: 0.552770, acc: 73.44%][G loss: 0.891555]\n",
      "[Epoch 4][Iteration 90][D loss: 0.658998, acc: 66.41%][G loss: 0.967746]\n",
      "[Epoch 4][Iteration 100][D loss: 0.599859, acc: 67.97%][G loss: 0.977538]\n",
      "[Epoch 4][Iteration 110][D loss: 0.528165, acc: 74.22%][G loss: 1.017205]\n",
      "[Epoch 4][Iteration 120][D loss: 0.755803, acc: 60.16%][G loss: 0.999820]\n",
      "[Epoch 4][Iteration 130][D loss: 0.804677, acc: 51.56%][G loss: 0.788028]\n",
      "[Epoch 4][Iteration 140][D loss: 0.607591, acc: 70.31%][G loss: 0.949301]\n",
      "[Epoch 4][Iteration 150][D loss: 0.621382, acc: 65.62%][G loss: 0.830951]\n",
      "[Epoch 5][Iteration 0][D loss: 0.616275, acc: 64.84%][G loss: 0.807067]\n",
      "[Epoch 5][Iteration 10][D loss: 0.603211, acc: 68.75%][G loss: 0.767741]\n",
      "[Epoch 5][Iteration 20][D loss: 0.555544, acc: 70.31%][G loss: 0.747062]\n",
      "[Epoch 5][Iteration 30][D loss: 0.568666, acc: 69.53%][G loss: 0.797570]\n",
      "[Epoch 5][Iteration 40][D loss: 0.560627, acc: 71.09%][G loss: 0.771965]\n",
      "[Epoch 5][Iteration 50][D loss: 0.656472, acc: 58.59%][G loss: 0.725487]\n",
      "[Epoch 5][Iteration 60][D loss: 0.511619, acc: 74.22%][G loss: 0.825683]\n",
      "[Epoch 5][Iteration 70][D loss: 0.624804, acc: 62.50%][G loss: 0.797959]\n",
      "[Epoch 5][Iteration 80][D loss: 0.649297, acc: 69.53%][G loss: 0.890254]\n",
      "[Epoch 5][Iteration 90][D loss: 0.526689, acc: 66.41%][G loss: 0.868732]\n",
      "[Epoch 5][Iteration 100][D loss: 0.475336, acc: 69.53%][G loss: 0.921641]\n",
      "[Epoch 5][Iteration 110][D loss: 0.503946, acc: 72.66%][G loss: 0.916122]\n",
      "[Epoch 5][Iteration 120][D loss: 0.555195, acc: 73.44%][G loss: 0.804774]\n",
      "[Epoch 5][Iteration 130][D loss: 0.561703, acc: 72.66%][G loss: 0.854466]\n",
      "[Epoch 5][Iteration 140][D loss: 0.456180, acc: 74.22%][G loss: 0.863962]\n",
      "[Epoch 5][Iteration 150][D loss: 0.499073, acc: 76.56%][G loss: 0.784462]\n",
      "[Epoch 6][Iteration 0][D loss: 0.510011, acc: 72.66%][G loss: 0.974749]\n",
      "[Epoch 6][Iteration 10][D loss: 0.550298, acc: 70.31%][G loss: 0.877931]\n",
      "[Epoch 6][Iteration 20][D loss: 0.376977, acc: 86.72%][G loss: 0.959643]\n",
      "[Epoch 6][Iteration 30][D loss: 0.475687, acc: 80.47%][G loss: 0.962133]\n",
      "[Epoch 6][Iteration 40][D loss: 0.584162, acc: 70.31%][G loss: 0.933132]\n",
      "[Epoch 6][Iteration 50][D loss: 0.681290, acc: 70.31%][G loss: 0.841983]\n",
      "[Epoch 6][Iteration 60][D loss: 0.596585, acc: 64.06%][G loss: 0.771364]\n",
      "[Epoch 6][Iteration 70][D loss: 0.494049, acc: 78.12%][G loss: 0.920701]\n",
      "[Epoch 6][Iteration 80][D loss: 0.534669, acc: 75.00%][G loss: 0.916305]\n",
      "[Epoch 6][Iteration 90][D loss: 0.648807, acc: 59.38%][G loss: 1.019891]\n",
      "[Epoch 6][Iteration 100][D loss: 0.616643, acc: 68.75%][G loss: 1.009592]\n",
      "[Epoch 6][Iteration 110][D loss: 0.596829, acc: 71.88%][G loss: 0.864753]\n",
      "[Epoch 6][Iteration 120][D loss: 0.744155, acc: 62.50%][G loss: 1.063567]\n",
      "[Epoch 6][Iteration 130][D loss: 0.793590, acc: 55.47%][G loss: 0.795839]\n",
      "[Epoch 6][Iteration 140][D loss: 0.667330, acc: 67.19%][G loss: 0.843781]\n",
      "[Epoch 6][Iteration 150][D loss: 0.730928, acc: 69.53%][G loss: 1.004539]\n",
      "[Epoch 7][Iteration 0][D loss: 0.665114, acc: 69.53%][G loss: 0.857418]\n",
      "[Epoch 7][Iteration 10][D loss: 0.565331, acc: 71.88%][G loss: 1.008468]\n",
      "[Epoch 7][Iteration 20][D loss: 0.637532, acc: 64.06%][G loss: 0.860840]\n",
      "[Epoch 7][Iteration 30][D loss: 0.635705, acc: 68.75%][G loss: 0.840823]\n",
      "[Epoch 7][Iteration 40][D loss: 0.660996, acc: 64.84%][G loss: 0.770301]\n",
      "[Epoch 7][Iteration 50][D loss: 0.673617, acc: 64.06%][G loss: 0.891433]\n",
      "[Epoch 7][Iteration 60][D loss: 0.649340, acc: 67.19%][G loss: 0.670925]\n",
      "[Epoch 7][Iteration 70][D loss: 0.743596, acc: 58.59%][G loss: 0.725681]\n",
      "[Epoch 7][Iteration 80][D loss: 0.668738, acc: 62.50%][G loss: 0.766718]\n",
      "[Epoch 7][Iteration 90][D loss: 0.622698, acc: 67.19%][G loss: 0.793652]\n",
      "[Epoch 7][Iteration 100][D loss: 0.578282, acc: 67.19%][G loss: 0.774246]\n",
      "[Epoch 7][Iteration 110][D loss: 0.500120, acc: 75.78%][G loss: 0.864506]\n",
      "[Epoch 7][Iteration 120][D loss: 0.566210, acc: 70.31%][G loss: 0.877796]\n",
      "[Epoch 7][Iteration 130][D loss: 0.456574, acc: 80.47%][G loss: 0.881033]\n",
      "[Epoch 7][Iteration 140][D loss: 0.624853, acc: 69.53%][G loss: 0.971911]\n",
      "[Epoch 7][Iteration 150][D loss: 0.627884, acc: 67.19%][G loss: 0.952760]\n",
      "[Epoch 8][Iteration 0][D loss: 1.155939, acc: 52.34%][G loss: 0.634879]\n",
      "[Epoch 8][Iteration 10][D loss: 0.706626, acc: 63.28%][G loss: 0.871185]\n",
      "[Epoch 8][Iteration 20][D loss: 0.651126, acc: 67.97%][G loss: 0.841931]\n",
      "[Epoch 8][Iteration 30][D loss: 0.746656, acc: 58.59%][G loss: 0.821551]\n",
      "[Epoch 8][Iteration 40][D loss: 0.691965, acc: 65.62%][G loss: 0.872194]\n",
      "[Epoch 8][Iteration 50][D loss: 0.690413, acc: 65.62%][G loss: 0.801374]\n",
      "[Epoch 8][Iteration 60][D loss: 0.757684, acc: 58.59%][G loss: 0.814833]\n",
      "[Epoch 8][Iteration 70][D loss: 0.611397, acc: 67.97%][G loss: 0.859326]\n",
      "[Epoch 8][Iteration 80][D loss: 0.602511, acc: 65.62%][G loss: 0.717542]\n",
      "[Epoch 8][Iteration 90][D loss: 0.666357, acc: 61.72%][G loss: 0.755622]\n",
      "[Epoch 8][Iteration 100][D loss: 0.661813, acc: 61.72%][G loss: 0.836882]\n",
      "[Epoch 8][Iteration 110][D loss: 0.626429, acc: 65.62%][G loss: 0.809116]\n",
      "[Epoch 8][Iteration 120][D loss: 0.626984, acc: 60.94%][G loss: 0.746283]\n",
      "[Epoch 8][Iteration 130][D loss: 0.584219, acc: 64.84%][G loss: 0.753451]\n",
      "[Epoch 8][Iteration 140][D loss: 0.615428, acc: 67.19%][G loss: 0.787055]\n",
      "[Epoch 8][Iteration 150][D loss: 0.595441, acc: 69.53%][G loss: 0.727238]\n",
      "[Epoch 9][Iteration 0][D loss: 0.673399, acc: 59.38%][G loss: 0.641229]\n",
      "[Epoch 9][Iteration 10][D loss: 0.669923, acc: 59.38%][G loss: 0.720686]\n",
      "[Epoch 9][Iteration 20][D loss: 0.698542, acc: 54.69%][G loss: 0.846446]\n",
      "[Epoch 9][Iteration 30][D loss: 0.662231, acc: 58.59%][G loss: 0.758406]\n",
      "[Epoch 9][Iteration 40][D loss: 0.773117, acc: 50.78%][G loss: 0.798237]\n",
      "[Epoch 9][Iteration 50][D loss: 0.612994, acc: 64.84%][G loss: 0.716514]\n",
      "[Epoch 9][Iteration 60][D loss: 0.610371, acc: 71.88%][G loss: 0.696902]\n",
      "[Epoch 9][Iteration 70][D loss: 0.604270, acc: 67.97%][G loss: 0.780642]\n",
      "[Epoch 9][Iteration 80][D loss: 0.619999, acc: 68.75%][G loss: 0.776198]\n",
      "[Epoch 9][Iteration 90][D loss: 0.608357, acc: 63.28%][G loss: 0.686607]\n",
      "[Epoch 9][Iteration 100][D loss: 0.696610, acc: 56.25%][G loss: 0.706274]\n",
      "[Epoch 9][Iteration 110][D loss: 0.639051, acc: 67.19%][G loss: 0.707909]\n",
      "[Epoch 9][Iteration 120][D loss: 0.631998, acc: 60.94%][G loss: 0.740750]\n",
      "[Epoch 9][Iteration 130][D loss: 0.715284, acc: 54.69%][G loss: 0.733698]\n",
      "[Epoch 9][Iteration 140][D loss: 0.693879, acc: 62.50%][G loss: 0.723988]\n",
      "[Epoch 9][Iteration 150][D loss: 0.623967, acc: 60.16%][G loss: 0.746133]\n",
      "[Epoch 10][Iteration 0][D loss: 0.639780, acc: 61.72%][G loss: 0.770445]\n",
      "[Epoch 10][Iteration 10][D loss: 0.633707, acc: 57.03%][G loss: 0.764085]\n",
      "[Epoch 10][Iteration 20][D loss: 0.612797, acc: 64.06%][G loss: 0.714737]\n",
      "[Epoch 10][Iteration 30][D loss: 0.598447, acc: 64.06%][G loss: 0.774707]\n",
      "[Epoch 10][Iteration 40][D loss: 0.620532, acc: 64.84%][G loss: 0.740667]\n",
      "[Epoch 10][Iteration 50][D loss: 0.599161, acc: 60.94%][G loss: 0.806768]\n",
      "[Epoch 10][Iteration 60][D loss: 0.594991, acc: 66.41%][G loss: 0.785325]\n",
      "[Epoch 10][Iteration 70][D loss: 0.582322, acc: 68.75%][G loss: 0.733598]\n",
      "[Epoch 10][Iteration 80][D loss: 0.582398, acc: 71.09%][G loss: 0.750748]\n",
      "[Epoch 10][Iteration 90][D loss: 0.660560, acc: 62.50%][G loss: 0.701359]\n",
      "[Epoch 10][Iteration 100][D loss: 0.640178, acc: 59.38%][G loss: 0.744999]\n",
      "[Epoch 10][Iteration 110][D loss: 0.623960, acc: 60.16%][G loss: 0.709361]\n",
      "[Epoch 10][Iteration 120][D loss: 0.590156, acc: 67.19%][G loss: 0.728886]\n",
      "[Epoch 10][Iteration 130][D loss: 0.646588, acc: 58.59%][G loss: 0.730363]\n",
      "[Epoch 10][Iteration 140][D loss: 0.677587, acc: 59.38%][G loss: 0.685837]\n",
      "[Epoch 10][Iteration 150][D loss: 0.655079, acc: 63.28%][G loss: 0.736386]\n",
      "[Epoch 11][Iteration 0][D loss: 0.583420, acc: 64.06%][G loss: 0.740704]\n",
      "[Epoch 11][Iteration 10][D loss: 0.636537, acc: 65.62%][G loss: 0.708250]\n",
      "[Epoch 11][Iteration 20][D loss: 0.651200, acc: 61.72%][G loss: 0.704307]\n",
      "[Epoch 11][Iteration 30][D loss: 0.733788, acc: 55.47%][G loss: 0.719108]\n",
      "[Epoch 11][Iteration 40][D loss: 0.606330, acc: 64.06%][G loss: 0.731946]\n",
      "[Epoch 11][Iteration 50][D loss: 0.666379, acc: 57.03%][G loss: 0.745709]\n",
      "[Epoch 11][Iteration 60][D loss: 0.606609, acc: 64.84%][G loss: 0.712455]\n",
      "[Epoch 11][Iteration 70][D loss: 0.647770, acc: 57.81%][G loss: 0.785936]\n",
      "[Epoch 11][Iteration 80][D loss: 0.617065, acc: 65.62%][G loss: 0.789664]\n",
      "[Epoch 11][Iteration 90][D loss: 0.552502, acc: 75.00%][G loss: 0.845993]\n",
      "[Epoch 11][Iteration 100][D loss: 0.654878, acc: 59.38%][G loss: 0.710059]\n",
      "[Epoch 11][Iteration 110][D loss: 0.646887, acc: 60.16%][G loss: 0.698672]\n",
      "[Epoch 11][Iteration 120][D loss: 0.719483, acc: 62.50%][G loss: 0.771644]\n",
      "[Epoch 11][Iteration 130][D loss: 0.599989, acc: 60.16%][G loss: 0.738394]\n",
      "[Epoch 11][Iteration 140][D loss: 0.624902, acc: 68.75%][G loss: 0.870251]\n",
      "[Epoch 11][Iteration 150][D loss: 0.646418, acc: 61.72%][G loss: 0.739975]\n",
      "[Epoch 12][Iteration 0][D loss: 0.584884, acc: 66.41%][G loss: 0.711945]\n",
      "[Epoch 12][Iteration 10][D loss: 0.608960, acc: 67.97%][G loss: 0.766766]\n",
      "[Epoch 12][Iteration 20][D loss: 0.619567, acc: 68.75%][G loss: 0.745664]\n",
      "[Epoch 12][Iteration 30][D loss: 0.602152, acc: 62.50%][G loss: 0.757162]\n",
      "[Epoch 12][Iteration 40][D loss: 0.632109, acc: 65.62%][G loss: 0.750656]\n",
      "[Epoch 12][Iteration 50][D loss: 0.660573, acc: 60.16%][G loss: 0.840929]\n",
      "[Epoch 12][Iteration 60][D loss: 0.680701, acc: 62.50%][G loss: 0.762219]\n",
      "[Epoch 12][Iteration 70][D loss: 0.629949, acc: 56.25%][G loss: 0.812092]\n",
      "[Epoch 12][Iteration 80][D loss: 0.634211, acc: 57.03%][G loss: 0.764817]\n",
      "[Epoch 12][Iteration 90][D loss: 0.620170, acc: 61.72%][G loss: 0.804429]\n",
      "[Epoch 12][Iteration 100][D loss: 0.671116, acc: 60.94%][G loss: 0.780658]\n",
      "[Epoch 12][Iteration 110][D loss: 0.551589, acc: 70.31%][G loss: 0.762536]\n",
      "[Epoch 12][Iteration 120][D loss: 0.591521, acc: 66.41%][G loss: 0.758813]\n",
      "[Epoch 12][Iteration 130][D loss: 0.632194, acc: 60.16%][G loss: 0.703605]\n",
      "[Epoch 12][Iteration 140][D loss: 0.627032, acc: 66.41%][G loss: 0.826350]\n",
      "[Epoch 12][Iteration 150][D loss: 0.657050, acc: 58.59%][G loss: 0.796540]\n",
      "[Epoch 13][Iteration 0][D loss: 0.632910, acc: 60.16%][G loss: 0.762566]\n",
      "[Epoch 13][Iteration 10][D loss: 0.640880, acc: 61.72%][G loss: 0.794335]\n",
      "[Epoch 13][Iteration 20][D loss: 0.637248, acc: 65.62%][G loss: 0.795971]\n",
      "[Epoch 13][Iteration 30][D loss: 0.592059, acc: 64.06%][G loss: 0.850377]\n",
      "[Epoch 13][Iteration 40][D loss: 0.670570, acc: 57.03%][G loss: 0.802235]\n",
      "[Epoch 13][Iteration 50][D loss: 0.572119, acc: 65.62%][G loss: 0.789351]\n",
      "[Epoch 13][Iteration 60][D loss: 0.704224, acc: 51.56%][G loss: 0.782586]\n",
      "[Epoch 13][Iteration 70][D loss: 0.669870, acc: 60.94%][G loss: 0.795707]\n",
      "[Epoch 13][Iteration 80][D loss: 0.684394, acc: 57.81%][G loss: 0.747279]\n",
      "[Epoch 13][Iteration 90][D loss: 0.719028, acc: 56.25%][G loss: 0.692937]\n",
      "[Epoch 13][Iteration 100][D loss: 0.700295, acc: 55.47%][G loss: 0.723246]\n",
      "[Epoch 13][Iteration 110][D loss: 0.607157, acc: 63.28%][G loss: 0.752590]\n",
      "[Epoch 13][Iteration 120][D loss: 0.752154, acc: 57.81%][G loss: 0.946734]\n",
      "[Epoch 13][Iteration 130][D loss: 0.680707, acc: 61.72%][G loss: 0.744326]\n",
      "[Epoch 13][Iteration 140][D loss: 0.648667, acc: 62.50%][G loss: 0.770249]\n",
      "[Epoch 13][Iteration 150][D loss: 0.677712, acc: 57.81%][G loss: 0.793874]\n",
      "[Epoch 14][Iteration 0][D loss: 0.706264, acc: 53.91%][G loss: 0.805815]\n",
      "[Epoch 14][Iteration 10][D loss: 0.705753, acc: 53.91%][G loss: 0.761070]\n",
      "[Epoch 14][Iteration 20][D loss: 0.684793, acc: 61.72%][G loss: 0.750115]\n",
      "[Epoch 14][Iteration 30][D loss: 0.671204, acc: 58.59%][G loss: 0.745532]\n",
      "[Epoch 14][Iteration 40][D loss: 0.661464, acc: 64.06%][G loss: 0.780557]\n",
      "[Epoch 14][Iteration 50][D loss: 0.643145, acc: 65.62%][G loss: 0.772532]\n",
      "[Epoch 14][Iteration 60][D loss: 0.649877, acc: 65.62%][G loss: 0.785821]\n",
      "[Epoch 14][Iteration 70][D loss: 0.709431, acc: 52.34%][G loss: 0.780089]\n",
      "[Epoch 14][Iteration 80][D loss: 0.687445, acc: 57.03%][G loss: 0.811935]\n",
      "[Epoch 14][Iteration 90][D loss: 0.662338, acc: 59.38%][G loss: 0.809689]\n",
      "[Epoch 14][Iteration 100][D loss: 0.696305, acc: 54.69%][G loss: 0.816886]\n",
      "[Epoch 14][Iteration 110][D loss: 0.689231, acc: 57.81%][G loss: 0.759765]\n",
      "[Epoch 14][Iteration 120][D loss: 0.714107, acc: 53.12%][G loss: 0.808329]\n",
      "[Epoch 14][Iteration 130][D loss: 0.694777, acc: 58.59%][G loss: 0.767255]\n",
      "[Epoch 14][Iteration 140][D loss: 0.577754, acc: 67.19%][G loss: 0.746882]\n",
      "[Epoch 14][Iteration 150][D loss: 0.587984, acc: 67.97%][G loss: 0.795316]\n",
      "[Epoch 15][Iteration 0][D loss: 0.663547, acc: 67.19%][G loss: 0.822061]\n",
      "[Epoch 15][Iteration 10][D loss: 0.648263, acc: 61.72%][G loss: 0.824999]\n",
      "[Epoch 15][Iteration 20][D loss: 0.684670, acc: 60.16%][G loss: 0.758690]\n",
      "[Epoch 15][Iteration 30][D loss: 0.682705, acc: 61.72%][G loss: 0.787256]\n",
      "[Epoch 15][Iteration 40][D loss: 0.598841, acc: 67.19%][G loss: 0.802622]\n",
      "[Epoch 15][Iteration 50][D loss: 0.628093, acc: 64.06%][G loss: 0.762599]\n",
      "[Epoch 15][Iteration 60][D loss: 0.686719, acc: 64.06%][G loss: 0.843977]\n",
      "[Epoch 15][Iteration 70][D loss: 0.647716, acc: 57.03%][G loss: 0.776188]\n",
      "[Epoch 15][Iteration 80][D loss: 0.658732, acc: 62.50%][G loss: 0.828548]\n",
      "[Epoch 15][Iteration 90][D loss: 0.641263, acc: 57.81%][G loss: 0.790121]\n",
      "[Epoch 15][Iteration 100][D loss: 0.688991, acc: 50.78%][G loss: 0.757597]\n",
      "[Epoch 15][Iteration 110][D loss: 0.687001, acc: 58.59%][G loss: 0.769026]\n",
      "[Epoch 15][Iteration 120][D loss: 0.715829, acc: 52.34%][G loss: 0.769673]\n",
      "[Epoch 15][Iteration 130][D loss: 0.652597, acc: 59.38%][G loss: 0.768131]\n",
      "[Epoch 15][Iteration 140][D loss: 0.669363, acc: 57.81%][G loss: 0.777735]\n",
      "[Epoch 15][Iteration 150][D loss: 0.712231, acc: 57.03%][G loss: 0.832484]\n",
      "[Epoch 16][Iteration 0][D loss: 0.720713, acc: 50.78%][G loss: 0.819561]\n",
      "[Epoch 16][Iteration 10][D loss: 0.664938, acc: 65.62%][G loss: 0.761328]\n",
      "[Epoch 16][Iteration 20][D loss: 0.710949, acc: 49.22%][G loss: 0.765333]\n",
      "[Epoch 16][Iteration 30][D loss: 0.639204, acc: 65.62%][G loss: 0.713544]\n",
      "[Epoch 16][Iteration 40][D loss: 0.688078, acc: 57.03%][G loss: 0.739476]\n",
      "[Epoch 16][Iteration 50][D loss: 0.664215, acc: 60.16%][G loss: 0.742749]\n",
      "[Epoch 16][Iteration 60][D loss: 0.648329, acc: 58.59%][G loss: 0.751183]\n",
      "[Epoch 16][Iteration 70][D loss: 0.690880, acc: 53.91%][G loss: 0.693884]\n",
      "[Epoch 16][Iteration 80][D loss: 0.659092, acc: 60.16%][G loss: 0.790043]\n",
      "[Epoch 16][Iteration 90][D loss: 0.650646, acc: 59.38%][G loss: 0.710210]\n",
      "[Epoch 16][Iteration 100][D loss: 0.671825, acc: 55.47%][G loss: 0.753605]\n",
      "[Epoch 16][Iteration 110][D loss: 0.635248, acc: 60.94%][G loss: 0.766733]\n",
      "[Epoch 16][Iteration 120][D loss: 0.642825, acc: 60.16%][G loss: 0.801552]\n",
      "[Epoch 16][Iteration 130][D loss: 0.609322, acc: 64.84%][G loss: 0.794067]\n",
      "[Epoch 16][Iteration 140][D loss: 0.669193, acc: 62.50%][G loss: 0.806284]\n",
      "[Epoch 16][Iteration 150][D loss: 0.634412, acc: 64.84%][G loss: 0.790111]\n",
      "[Epoch 17][Iteration 0][D loss: 0.739375, acc: 54.69%][G loss: 0.756451]\n",
      "[Epoch 17][Iteration 10][D loss: 0.633724, acc: 61.72%][G loss: 0.810512]\n",
      "[Epoch 17][Iteration 20][D loss: 0.674388, acc: 60.16%][G loss: 0.743831]\n",
      "[Epoch 17][Iteration 30][D loss: 0.646269, acc: 58.59%][G loss: 0.786762]\n",
      "[Epoch 17][Iteration 40][D loss: 0.648253, acc: 60.16%][G loss: 0.803718]\n",
      "[Epoch 17][Iteration 50][D loss: 0.725075, acc: 57.81%][G loss: 0.830327]\n",
      "[Epoch 17][Iteration 60][D loss: 0.677861, acc: 56.25%][G loss: 0.785751]\n",
      "[Epoch 17][Iteration 70][D loss: 0.669939, acc: 57.81%][G loss: 0.756841]\n",
      "[Epoch 17][Iteration 80][D loss: 0.631423, acc: 63.28%][G loss: 0.742950]\n",
      "[Epoch 17][Iteration 90][D loss: 0.600932, acc: 67.19%][G loss: 0.810544]\n",
      "[Epoch 17][Iteration 100][D loss: 0.747139, acc: 56.25%][G loss: 0.698988]\n",
      "[Epoch 17][Iteration 110][D loss: 0.741150, acc: 59.38%][G loss: 0.817238]\n",
      "[Epoch 17][Iteration 120][D loss: 0.680297, acc: 56.25%][G loss: 0.799347]\n",
      "[Epoch 17][Iteration 130][D loss: 0.693740, acc: 56.25%][G loss: 0.777104]\n",
      "[Epoch 17][Iteration 140][D loss: 0.642736, acc: 60.94%][G loss: 0.780942]\n",
      "[Epoch 17][Iteration 150][D loss: 0.711908, acc: 59.38%][G loss: 0.799297]\n",
      "[Epoch 18][Iteration 0][D loss: 0.653220, acc: 55.47%][G loss: 0.758921]\n",
      "[Epoch 18][Iteration 10][D loss: 0.639951, acc: 59.38%][G loss: 0.760602]\n",
      "[Epoch 18][Iteration 20][D loss: 0.671868, acc: 59.38%][G loss: 0.752667]\n",
      "[Epoch 18][Iteration 30][D loss: 0.689994, acc: 60.16%][G loss: 0.759833]\n",
      "[Epoch 18][Iteration 40][D loss: 0.704310, acc: 51.56%][G loss: 0.823151]\n",
      "[Epoch 18][Iteration 50][D loss: 0.697165, acc: 54.69%][G loss: 0.791439]\n",
      "[Epoch 18][Iteration 60][D loss: 0.648211, acc: 62.50%][G loss: 0.793605]\n",
      "[Epoch 18][Iteration 70][D loss: 0.648590, acc: 58.59%][G loss: 0.770318]\n",
      "[Epoch 18][Iteration 80][D loss: 0.743236, acc: 52.34%][G loss: 0.796231]\n",
      "[Epoch 18][Iteration 90][D loss: 0.729290, acc: 57.81%][G loss: 0.778521]\n",
      "[Epoch 18][Iteration 100][D loss: 0.648616, acc: 60.16%][G loss: 0.816826]\n",
      "[Epoch 18][Iteration 110][D loss: 0.754997, acc: 50.78%][G loss: 0.815958]\n",
      "[Epoch 18][Iteration 120][D loss: 0.708056, acc: 50.00%][G loss: 0.738018]\n",
      "[Epoch 18][Iteration 130][D loss: 0.669343, acc: 58.59%][G loss: 0.763492]\n",
      "[Epoch 18][Iteration 140][D loss: 0.661298, acc: 60.94%][G loss: 0.737720]\n",
      "[Epoch 18][Iteration 150][D loss: 0.699985, acc: 53.91%][G loss: 0.709076]\n",
      "[Epoch 19][Iteration 0][D loss: 0.762963, acc: 55.47%][G loss: 0.673049]\n",
      "[Epoch 19][Iteration 10][D loss: 0.798664, acc: 47.66%][G loss: 0.710892]\n",
      "[Epoch 19][Iteration 20][D loss: 0.703964, acc: 56.25%][G loss: 0.721901]\n",
      "[Epoch 19][Iteration 30][D loss: 0.763575, acc: 53.12%][G loss: 0.798717]\n",
      "[Epoch 19][Iteration 40][D loss: 0.726321, acc: 50.00%][G loss: 0.728261]\n",
      "[Epoch 19][Iteration 50][D loss: 0.726092, acc: 50.78%][G loss: 0.770347]\n",
      "[Epoch 19][Iteration 60][D loss: 0.757300, acc: 52.34%][G loss: 0.711488]\n",
      "[Epoch 19][Iteration 70][D loss: 0.676738, acc: 61.72%][G loss: 0.695055]\n",
      "[Epoch 19][Iteration 80][D loss: 0.716808, acc: 55.47%][G loss: 0.826914]\n",
      "[Epoch 19][Iteration 90][D loss: 0.714319, acc: 53.12%][G loss: 0.800458]\n",
      "[Epoch 19][Iteration 100][D loss: 0.624993, acc: 63.28%][G loss: 0.711512]\n",
      "[Epoch 19][Iteration 110][D loss: 0.638820, acc: 60.94%][G loss: 0.736621]\n",
      "[Epoch 19][Iteration 120][D loss: 0.703164, acc: 55.47%][G loss: 0.757788]\n",
      "[Epoch 19][Iteration 130][D loss: 0.736038, acc: 54.69%][G loss: 0.800409]\n",
      "[Epoch 19][Iteration 140][D loss: 0.696267, acc: 57.81%][G loss: 0.724982]\n",
      "[Epoch 19][Iteration 150][D loss: 0.715374, acc: 59.38%][G loss: 0.776057]\n",
      "Wall time: 13min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gan = GAN(128, img_size=(64, 64, 3))\n",
    "gan.connect()\n",
    "gan.train(20, 64, sample_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
