{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用昨天的卡通資料集來練GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, UpSampling2D, Dense, Flatten, Input, BatchNormalization, Reshape, LeakyReLU, Conv2DTranspose, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, folder_path, img_size):\n",
    "        self.folder_path = folder_path\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.path_list = glob(folder_path) # 讀取資料夾全部圖片路徑\n",
    "        assert len(self.path_list) > 0, 'path not existed!'\n",
    "    \n",
    "    def __imread(self, img_path):\n",
    "        '''讀取圖片'''\n",
    "        return np.array(Image.open(img_path).convert('RGB').resize(self.img_size[:-1], Image.ANTIALIAS))\n",
    "    \n",
    "    def sampling_data(self, batch_size, shuffle=True):\n",
    "        img_path_list = self.path_list\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(img_path_list)\n",
    "            \n",
    "        for batch_idx in range(0, len(img_path_list), batch_size):\n",
    "            path_set = img_path_list[batch_idx : batch_idx + batch_size]\n",
    "            \n",
    "            # 預設空間，避免 append很慢\n",
    "            img_set = np.zeros((len(path_set),) + self.img_size)\n",
    "            for img_idx, path in enumerate(path_set):\n",
    "                img_set[img_idx] = self.__imread(path)\n",
    "            \n",
    "            # 127.5是255的一半，一到負一之間\n",
    "            img_set = img_set / 127.5 - 1\n",
    "            # 暫停\n",
    "            yield img_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, noise_dim, img_size=(64, 64, 3)):\n",
    "        self.noise_dim = noise_dim # noise_dim = 雜訊維度\n",
    "        self.img_size = img_size # img_size = 圖片大小\n",
    "        self.dataloader = DataLoader('../0709/Preview/cartoon/*.png', self.img_size)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise_input = Input(shape=(self.noise_dim,))\n",
    "        \n",
    "        x = Dense(128)(noise_input)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        \n",
    "#         x = Dense(256)(x)\n",
    "#         x = BatchNormalization(momentum=0.8)(x)\n",
    "#         x = LeakyReLU(.2)(x)\n",
    "        \n",
    "        x = Dense(128)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        \n",
    "        x = Dense(64*64*3, activation='tanh')(x)\n",
    "        \n",
    "        img = Reshape((64, 64, 3))(x)\n",
    "\n",
    "        generator = Model(noise_input, img)\n",
    "#         generator.summary()\n",
    "        return generator\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_input = Input(shape=self.img_size)\n",
    "        # 把照片拉直好放入dense\n",
    "        x = Flatten()(img_input)\n",
    "        \n",
    "        x = Dense(32)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        \n",
    "        x = Dense(32)(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(.2)(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        # 重要的技巧（新增一個dropout層）\n",
    "#         x = Dropout(0.4)(x)\n",
    "\n",
    "        # 分類層\n",
    "        validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        discriminator = Model(img_input, validity)\n",
    "#         discriminator.summary()\n",
    "        return discriminator\n",
    "\n",
    "    def connect(self):\n",
    "        self.generator = self.build_generator()\n",
    "        print(self.generator.count_params())\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        print(self.discriminator.count_params())\n",
    "        self.optimizer = Adam(.0002, .5)\n",
    "        # Optimizer用Adam, Learning rate=0.0001~0.0002, 切勿調高\n",
    "        self.discriminator.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "        \n",
    "        noise = Input(shape=(self.noise_dim,))\n",
    "        img = self.generator(noise)\n",
    "        self.discriminator.trainable = False # 在訓練G時, 鎖定D\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(noise, validity)\n",
    "        self.combined.compile(optimizer=self.optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=200):\n",
    "        self.history = []\n",
    "        valid = np.ones((batch_size, 1)) # 1 = 真實圖片\n",
    "        fake = np.zeros((batch_size, 1)) # 0 = 生成圖片\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for i, real_img in enumerate(self.dataloader.sampling_data(batch_size)):\n",
    "                # Train D\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                fake_img = self.generator.predict(noise)\n",
    "\n",
    "                d_loss_real, real_acc = self.discriminator.train_on_batch(real_img, valid[:len(real_img)])\n",
    "                d_loss_fake, fake_acc = self.discriminator.train_on_batch(fake_img, fake)\n",
    "                d_loss = .5 * (d_loss_real + d_loss_fake)\n",
    "                d_acc = .5 * (real_acc + fake_acc)\n",
    "                                                                          \n",
    "                # Train G\n",
    "                noise = np.random.standard_normal((batch_size, self.noise_dim))\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                if i % sample_interval == 0:\n",
    "                    info = {\n",
    "                            'epoch': e,\n",
    "                            'iter': i,\n",
    "                            'd_loss': d_loss,\n",
    "                            'd_acc': d_acc*100,\n",
    "                            'g_loss': g_loss\n",
    "                            }\n",
    "                    self.history.append(list(info.values()))\n",
    "                    print('[Epoch %(epoch)d][Iteration %(iter)d][D loss: %(d_loss).6f, acc: %(d_acc).2f%%][G loss: %(g_loss).6f]' % info)\n",
    "            self.__sample_image(e)\n",
    "        return self.history\n",
    "\n",
    "    def __sample_image(self, epoch):\n",
    "        r, c = 8, 8 # 列, 欄\n",
    "        noise = np.random.standard_normal((r*c, self.noise_dim))\n",
    "        img = self.generator.predict(noise).reshape((r, c) + self.img_size)\n",
    "        img = img * .5 + .5\n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        axs = fig.subplots(r, c)\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(img[i, j])\n",
    "                axs[i, j].axis('off')\n",
    "        fig.savefig('./Image/%5d.png' % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 22:28:46.886754  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0710 22:28:46.907757  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0710 22:28:46.913855  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0710 22:28:46.969827  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0710 22:28:47.093827  9220 deprecation.py:506] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0710 22:28:47.164827  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0710 22:28:47.168827  9220 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0710 22:28:47.171828  9220 deprecation.py:323] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619200\n",
      "394593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][Iteration 0][D loss: 0.804137, acc: 50.78%][G loss: 0.776527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0][Iteration 10][D loss: 0.723090, acc: 53.91%][G loss: 0.706959]\n",
      "[Epoch 0][Iteration 20][D loss: 0.699262, acc: 57.81%][G loss: 0.696584]\n",
      "[Epoch 0][Iteration 30][D loss: 0.696898, acc: 50.00%][G loss: 0.700328]\n",
      "[Epoch 0][Iteration 40][D loss: 0.671510, acc: 58.59%][G loss: 0.697652]\n",
      "[Epoch 0][Iteration 50][D loss: 0.674081, acc: 61.72%][G loss: 0.698633]\n",
      "[Epoch 0][Iteration 60][D loss: 0.669831, acc: 63.28%][G loss: 0.710950]\n",
      "[Epoch 0][Iteration 70][D loss: 0.642923, acc: 66.41%][G loss: 0.700099]\n",
      "[Epoch 0][Iteration 80][D loss: 0.629963, acc: 67.19%][G loss: 0.704208]\n",
      "[Epoch 0][Iteration 90][D loss: 0.651374, acc: 62.50%][G loss: 0.703498]\n",
      "[Epoch 0][Iteration 100][D loss: 0.629819, acc: 64.06%][G loss: 0.696750]\n",
      "[Epoch 0][Iteration 110][D loss: 0.629018, acc: 69.53%][G loss: 0.725711]\n",
      "[Epoch 0][Iteration 120][D loss: 0.639143, acc: 68.75%][G loss: 0.723700]\n",
      "[Epoch 0][Iteration 130][D loss: 0.616881, acc: 75.00%][G loss: 0.726039]\n",
      "[Epoch 0][Iteration 140][D loss: 0.621369, acc: 71.88%][G loss: 0.729787]\n",
      "[Epoch 0][Iteration 150][D loss: 0.641028, acc: 67.97%][G loss: 0.717999]\n",
      "[Epoch 1][Iteration 0][D loss: 0.639850, acc: 69.53%][G loss: 0.733837]\n",
      "[Epoch 1][Iteration 10][D loss: 0.625212, acc: 71.88%][G loss: 0.738192]\n",
      "[Epoch 1][Iteration 20][D loss: 0.613815, acc: 72.66%][G loss: 0.744472]\n",
      "[Epoch 1][Iteration 30][D loss: 0.610693, acc: 75.78%][G loss: 0.744579]\n",
      "[Epoch 1][Iteration 40][D loss: 0.609788, acc: 75.00%][G loss: 0.725883]\n",
      "[Epoch 1][Iteration 50][D loss: 0.625897, acc: 71.09%][G loss: 0.745875]\n",
      "[Epoch 1][Iteration 60][D loss: 0.594336, acc: 78.91%][G loss: 0.753306]\n",
      "[Epoch 1][Iteration 70][D loss: 0.603412, acc: 74.22%][G loss: 0.731019]\n",
      "[Epoch 1][Iteration 80][D loss: 0.603834, acc: 74.22%][G loss: 0.751463]\n",
      "[Epoch 1][Iteration 90][D loss: 0.603876, acc: 75.00%][G loss: 0.763618]\n",
      "[Epoch 1][Iteration 100][D loss: 0.613395, acc: 75.00%][G loss: 0.775965]\n",
      "[Epoch 1][Iteration 110][D loss: 0.598852, acc: 78.12%][G loss: 0.759805]\n",
      "[Epoch 1][Iteration 120][D loss: 0.591309, acc: 82.03%][G loss: 0.766034]\n",
      "[Epoch 1][Iteration 130][D loss: 0.583386, acc: 83.59%][G loss: 0.772583]\n",
      "[Epoch 1][Iteration 140][D loss: 0.599938, acc: 78.12%][G loss: 0.781915]\n",
      "[Epoch 1][Iteration 150][D loss: 0.579817, acc: 80.47%][G loss: 0.776684]\n",
      "[Epoch 2][Iteration 0][D loss: 0.574704, acc: 83.59%][G loss: 0.781648]\n",
      "[Epoch 2][Iteration 10][D loss: 0.581674, acc: 82.03%][G loss: 0.769254]\n",
      "[Epoch 2][Iteration 20][D loss: 0.603757, acc: 80.47%][G loss: 0.772040]\n",
      "[Epoch 2][Iteration 30][D loss: 0.590179, acc: 78.12%][G loss: 0.781760]\n",
      "[Epoch 2][Iteration 40][D loss: 0.652197, acc: 65.62%][G loss: 0.793072]\n",
      "[Epoch 2][Iteration 50][D loss: 0.600390, acc: 81.25%][G loss: 0.791722]\n",
      "[Epoch 2][Iteration 60][D loss: 0.608430, acc: 77.34%][G loss: 0.791606]\n",
      "[Epoch 2][Iteration 70][D loss: 0.571435, acc: 85.94%][G loss: 0.779657]\n",
      "[Epoch 2][Iteration 80][D loss: 0.596250, acc: 85.94%][G loss: 0.812402]\n",
      "[Epoch 2][Iteration 90][D loss: 0.576160, acc: 82.03%][G loss: 0.802883]\n",
      "[Epoch 2][Iteration 100][D loss: 0.600056, acc: 78.12%][G loss: 0.817354]\n",
      "[Epoch 2][Iteration 110][D loss: 0.592300, acc: 85.16%][G loss: 0.827276]\n",
      "[Epoch 2][Iteration 120][D loss: 0.591093, acc: 82.81%][G loss: 0.806025]\n",
      "[Epoch 2][Iteration 130][D loss: 0.551070, acc: 83.59%][G loss: 0.828197]\n",
      "[Epoch 2][Iteration 140][D loss: 0.573748, acc: 83.59%][G loss: 0.789934]\n",
      "[Epoch 2][Iteration 150][D loss: 0.576223, acc: 82.81%][G loss: 0.811206]\n",
      "[Epoch 3][Iteration 0][D loss: 0.591862, acc: 79.69%][G loss: 0.812806]\n",
      "[Epoch 3][Iteration 10][D loss: 0.558614, acc: 89.84%][G loss: 0.836061]\n",
      "[Epoch 3][Iteration 20][D loss: 0.574614, acc: 85.16%][G loss: 0.841040]\n",
      "[Epoch 3][Iteration 30][D loss: 0.544532, acc: 86.72%][G loss: 0.830056]\n",
      "[Epoch 3][Iteration 40][D loss: 0.563225, acc: 86.72%][G loss: 0.815112]\n",
      "[Epoch 3][Iteration 50][D loss: 0.588003, acc: 85.16%][G loss: 0.818073]\n",
      "[Epoch 3][Iteration 60][D loss: 0.578475, acc: 82.03%][G loss: 0.797772]\n",
      "[Epoch 3][Iteration 70][D loss: 0.589612, acc: 80.47%][G loss: 0.811180]\n",
      "[Epoch 3][Iteration 80][D loss: 0.525413, acc: 85.94%][G loss: 0.816292]\n",
      "[Epoch 3][Iteration 90][D loss: 0.568461, acc: 78.91%][G loss: 0.827910]\n",
      "[Epoch 3][Iteration 100][D loss: 0.547603, acc: 85.94%][G loss: 0.806045]\n",
      "[Epoch 3][Iteration 110][D loss: 0.563781, acc: 85.94%][G loss: 0.843372]\n",
      "[Epoch 3][Iteration 120][D loss: 0.554070, acc: 87.50%][G loss: 0.811156]\n",
      "[Epoch 3][Iteration 130][D loss: 0.593880, acc: 78.12%][G loss: 0.815283]\n",
      "[Epoch 3][Iteration 140][D loss: 0.544083, acc: 90.62%][G loss: 0.829837]\n",
      "[Epoch 3][Iteration 150][D loss: 0.563264, acc: 89.06%][G loss: 0.861673]\n",
      "[Epoch 4][Iteration 0][D loss: 0.697567, acc: 59.38%][G loss: 0.815243]\n",
      "[Epoch 4][Iteration 10][D loss: 0.658527, acc: 64.06%][G loss: 0.834802]\n",
      "[Epoch 4][Iteration 20][D loss: 0.625142, acc: 75.00%][G loss: 0.738890]\n",
      "[Epoch 4][Iteration 30][D loss: 0.614519, acc: 71.88%][G loss: 0.836697]\n",
      "[Epoch 4][Iteration 40][D loss: 0.565434, acc: 85.94%][G loss: 0.825144]\n",
      "[Epoch 4][Iteration 50][D loss: 0.568440, acc: 84.38%][G loss: 0.818988]\n",
      "[Epoch 4][Iteration 60][D loss: 0.586295, acc: 80.47%][G loss: 0.827516]\n",
      "[Epoch 4][Iteration 70][D loss: 0.581989, acc: 81.25%][G loss: 0.786986]\n",
      "[Epoch 4][Iteration 80][D loss: 0.601400, acc: 76.56%][G loss: 0.832919]\n",
      "[Epoch 4][Iteration 90][D loss: 0.577915, acc: 84.38%][G loss: 0.828883]\n",
      "[Epoch 4][Iteration 100][D loss: 0.547004, acc: 86.72%][G loss: 0.859008]\n",
      "[Epoch 4][Iteration 110][D loss: 0.589829, acc: 78.12%][G loss: 0.874199]\n",
      "[Epoch 4][Iteration 120][D loss: 0.579311, acc: 78.91%][G loss: 0.854910]\n",
      "[Epoch 4][Iteration 130][D loss: 0.577243, acc: 80.47%][G loss: 0.846066]\n",
      "[Epoch 4][Iteration 140][D loss: 0.580058, acc: 82.81%][G loss: 0.850367]\n",
      "[Epoch 4][Iteration 150][D loss: 0.613716, acc: 78.91%][G loss: 0.820916]\n",
      "[Epoch 5][Iteration 0][D loss: 0.549563, acc: 86.72%][G loss: 0.855823]\n",
      "[Epoch 5][Iteration 10][D loss: 0.573441, acc: 84.38%][G loss: 0.801414]\n",
      "[Epoch 5][Iteration 20][D loss: 0.604583, acc: 78.91%][G loss: 0.855803]\n",
      "[Epoch 5][Iteration 30][D loss: 0.552662, acc: 85.16%][G loss: 0.810927]\n",
      "[Epoch 5][Iteration 40][D loss: 0.576361, acc: 80.47%][G loss: 0.846343]\n",
      "[Epoch 5][Iteration 50][D loss: 0.594095, acc: 79.69%][G loss: 0.853542]\n",
      "[Epoch 5][Iteration 60][D loss: 0.584120, acc: 84.38%][G loss: 0.845333]\n",
      "[Epoch 5][Iteration 70][D loss: 0.562818, acc: 82.81%][G loss: 0.877626]\n",
      "[Epoch 5][Iteration 80][D loss: 0.582447, acc: 77.34%][G loss: 0.833819]\n",
      "[Epoch 5][Iteration 90][D loss: 0.611083, acc: 76.56%][G loss: 0.843485]\n",
      "[Epoch 5][Iteration 100][D loss: 0.596503, acc: 85.16%][G loss: 0.861159]\n",
      "[Epoch 5][Iteration 110][D loss: 0.590267, acc: 84.38%][G loss: 0.807961]\n",
      "[Epoch 5][Iteration 120][D loss: 0.574543, acc: 80.47%][G loss: 0.794473]\n",
      "[Epoch 5][Iteration 130][D loss: 0.581355, acc: 80.47%][G loss: 0.825149]\n",
      "[Epoch 5][Iteration 140][D loss: 0.580312, acc: 83.59%][G loss: 0.837331]\n",
      "[Epoch 5][Iteration 150][D loss: 0.622796, acc: 67.19%][G loss: 0.892102]\n",
      "[Epoch 6][Iteration 0][D loss: 0.639377, acc: 73.44%][G loss: 0.775602]\n",
      "[Epoch 6][Iteration 10][D loss: 0.555943, acc: 89.84%][G loss: 0.780149]\n",
      "[Epoch 6][Iteration 20][D loss: 0.523130, acc: 89.06%][G loss: 0.812656]\n",
      "[Epoch 6][Iteration 30][D loss: 0.597955, acc: 77.34%][G loss: 0.878225]\n",
      "[Epoch 6][Iteration 40][D loss: 0.587176, acc: 78.91%][G loss: 0.835379]\n",
      "[Epoch 6][Iteration 50][D loss: 0.562281, acc: 87.50%][G loss: 0.866032]\n",
      "[Epoch 6][Iteration 60][D loss: 0.560891, acc: 81.25%][G loss: 0.838025]\n",
      "[Epoch 6][Iteration 70][D loss: 0.555762, acc: 85.94%][G loss: 0.850269]\n",
      "[Epoch 6][Iteration 80][D loss: 0.593131, acc: 83.59%][G loss: 0.858564]\n",
      "[Epoch 6][Iteration 90][D loss: 0.595443, acc: 71.09%][G loss: 0.872887]\n",
      "[Epoch 6][Iteration 100][D loss: 0.632836, acc: 58.59%][G loss: 0.887149]\n",
      "[Epoch 6][Iteration 110][D loss: 0.545192, acc: 88.28%][G loss: 0.817173]\n",
      "[Epoch 6][Iteration 120][D loss: 0.529050, acc: 89.06%][G loss: 0.832167]\n",
      "[Epoch 6][Iteration 130][D loss: 0.545489, acc: 85.94%][G loss: 0.788175]\n",
      "[Epoch 6][Iteration 140][D loss: 0.552287, acc: 84.38%][G loss: 0.740458]\n",
      "[Epoch 6][Iteration 150][D loss: 0.552420, acc: 85.94%][G loss: 0.789595]\n",
      "[Epoch 7][Iteration 0][D loss: 0.546847, acc: 85.16%][G loss: 0.834750]\n",
      "[Epoch 7][Iteration 10][D loss: 0.680962, acc: 60.94%][G loss: 0.908545]\n",
      "[Epoch 7][Iteration 20][D loss: 0.586646, acc: 82.81%][G loss: 0.836890]\n",
      "[Epoch 7][Iteration 30][D loss: 0.602059, acc: 74.22%][G loss: 0.849203]\n",
      "[Epoch 7][Iteration 40][D loss: 0.589152, acc: 82.81%][G loss: 0.842933]\n",
      "[Epoch 7][Iteration 50][D loss: 0.615576, acc: 78.91%][G loss: 0.794544]\n",
      "[Epoch 7][Iteration 60][D loss: 0.621398, acc: 70.31%][G loss: 0.787369]\n",
      "[Epoch 7][Iteration 70][D loss: 0.555978, acc: 88.28%][G loss: 0.850640]\n",
      "[Epoch 7][Iteration 80][D loss: 0.561406, acc: 89.84%][G loss: 0.910372]\n",
      "[Epoch 7][Iteration 90][D loss: 0.531089, acc: 89.84%][G loss: 0.872900]\n",
      "[Epoch 7][Iteration 100][D loss: 0.543093, acc: 87.50%][G loss: 0.791746]\n",
      "[Epoch 7][Iteration 110][D loss: 0.587874, acc: 81.25%][G loss: 0.870988]\n",
      "[Epoch 7][Iteration 120][D loss: 0.548239, acc: 89.84%][G loss: 0.844032]\n",
      "[Epoch 7][Iteration 130][D loss: 0.540913, acc: 85.94%][G loss: 0.882708]\n",
      "[Epoch 7][Iteration 140][D loss: 0.562085, acc: 87.50%][G loss: 0.884842]\n",
      "[Epoch 7][Iteration 150][D loss: 0.605393, acc: 81.25%][G loss: 0.833940]\n",
      "[Epoch 8][Iteration 0][D loss: 0.584206, acc: 84.38%][G loss: 0.858117]\n",
      "[Epoch 8][Iteration 10][D loss: 0.672057, acc: 62.50%][G loss: 0.843110]\n",
      "[Epoch 8][Iteration 20][D loss: 0.566283, acc: 84.38%][G loss: 0.837773]\n",
      "[Epoch 8][Iteration 30][D loss: 0.596327, acc: 75.00%][G loss: 0.848990]\n",
      "[Epoch 8][Iteration 40][D loss: 0.642216, acc: 69.53%][G loss: 0.927349]\n",
      "[Epoch 8][Iteration 50][D loss: 0.576840, acc: 77.34%][G loss: 0.855600]\n",
      "[Epoch 8][Iteration 60][D loss: 0.586105, acc: 78.12%][G loss: 0.849456]\n",
      "[Epoch 8][Iteration 70][D loss: 0.607069, acc: 75.78%][G loss: 0.832053]\n",
      "[Epoch 8][Iteration 80][D loss: 0.590783, acc: 78.91%][G loss: 0.908975]\n",
      "[Epoch 8][Iteration 90][D loss: 0.625932, acc: 77.34%][G loss: 0.813082]\n",
      "[Epoch 8][Iteration 100][D loss: 0.580656, acc: 82.81%][G loss: 0.839864]\n",
      "[Epoch 8][Iteration 110][D loss: 0.568136, acc: 83.59%][G loss: 0.914054]\n",
      "[Epoch 8][Iteration 120][D loss: 0.558601, acc: 87.50%][G loss: 0.807276]\n",
      "[Epoch 8][Iteration 130][D loss: 0.586313, acc: 85.16%][G loss: 0.768119]\n",
      "[Epoch 8][Iteration 140][D loss: 0.529613, acc: 85.94%][G loss: 0.943472]\n",
      "[Epoch 8][Iteration 150][D loss: 0.549989, acc: 89.84%][G loss: 0.868043]\n",
      "[Epoch 9][Iteration 0][D loss: 0.591806, acc: 74.22%][G loss: 0.867596]\n",
      "[Epoch 9][Iteration 10][D loss: 0.560874, acc: 82.81%][G loss: 0.869285]\n",
      "[Epoch 9][Iteration 20][D loss: 0.552321, acc: 88.28%][G loss: 0.847274]\n",
      "[Epoch 9][Iteration 30][D loss: 0.541151, acc: 89.84%][G loss: 0.862571]\n",
      "[Epoch 9][Iteration 40][D loss: 0.542388, acc: 87.50%][G loss: 0.811385]\n",
      "[Epoch 9][Iteration 50][D loss: 0.570414, acc: 85.94%][G loss: 0.876553]\n",
      "[Epoch 9][Iteration 60][D loss: 0.556566, acc: 87.50%][G loss: 0.845775]\n",
      "[Epoch 9][Iteration 70][D loss: 0.540481, acc: 88.28%][G loss: 0.867576]\n",
      "[Epoch 9][Iteration 80][D loss: 0.582010, acc: 85.16%][G loss: 0.846572]\n",
      "[Epoch 9][Iteration 90][D loss: 0.575694, acc: 83.59%][G loss: 0.842658]\n",
      "[Epoch 9][Iteration 100][D loss: 0.570077, acc: 83.59%][G loss: 0.771113]\n",
      "[Epoch 9][Iteration 110][D loss: 0.608034, acc: 78.91%][G loss: 0.862612]\n",
      "[Epoch 9][Iteration 120][D loss: 0.578662, acc: 85.94%][G loss: 0.701307]\n",
      "[Epoch 9][Iteration 130][D loss: 0.571465, acc: 79.69%][G loss: 0.875359]\n",
      "[Epoch 9][Iteration 140][D loss: 0.557568, acc: 85.94%][G loss: 0.901260]\n",
      "[Epoch 9][Iteration 150][D loss: 0.674768, acc: 60.16%][G loss: 0.838697]\n",
      "[Epoch 10][Iteration 0][D loss: 0.583992, acc: 73.44%][G loss: 0.863187]\n",
      "[Epoch 10][Iteration 10][D loss: 0.541570, acc: 85.94%][G loss: 0.848966]\n",
      "[Epoch 10][Iteration 20][D loss: 0.632957, acc: 69.53%][G loss: 0.846479]\n",
      "[Epoch 10][Iteration 30][D loss: 0.653102, acc: 74.22%][G loss: 0.899751]\n",
      "[Epoch 10][Iteration 40][D loss: 0.547126, acc: 88.28%][G loss: 0.747562]\n",
      "[Epoch 10][Iteration 50][D loss: 0.648302, acc: 70.31%][G loss: 0.884923]\n",
      "[Epoch 10][Iteration 60][D loss: 0.586629, acc: 81.25%][G loss: 0.778580]\n",
      "[Epoch 10][Iteration 70][D loss: 0.551338, acc: 88.28%][G loss: 0.754760]\n",
      "[Epoch 10][Iteration 80][D loss: 0.606712, acc: 79.69%][G loss: 0.906084]\n",
      "[Epoch 10][Iteration 90][D loss: 0.569452, acc: 87.50%][G loss: 0.848108]\n",
      "[Epoch 10][Iteration 100][D loss: 0.577985, acc: 80.47%][G loss: 0.862231]\n",
      "[Epoch 10][Iteration 110][D loss: 0.559008, acc: 84.38%][G loss: 0.911535]\n",
      "[Epoch 10][Iteration 120][D loss: 0.522388, acc: 88.28%][G loss: 0.789322]\n",
      "[Epoch 10][Iteration 130][D loss: 0.558198, acc: 83.59%][G loss: 0.921677]\n",
      "[Epoch 10][Iteration 140][D loss: 0.517442, acc: 91.41%][G loss: 1.020937]\n",
      "[Epoch 10][Iteration 150][D loss: 0.506613, acc: 93.75%][G loss: 0.764924]\n",
      "[Epoch 11][Iteration 0][D loss: 0.713678, acc: 60.16%][G loss: 0.817717]\n",
      "[Epoch 11][Iteration 10][D loss: 0.539179, acc: 88.28%][G loss: 0.781142]\n",
      "[Epoch 11][Iteration 20][D loss: 0.626786, acc: 68.75%][G loss: 0.902103]\n",
      "[Epoch 11][Iteration 30][D loss: 0.561736, acc: 87.50%][G loss: 0.844811]\n",
      "[Epoch 11][Iteration 40][D loss: 0.667498, acc: 64.84%][G loss: 0.770290]\n",
      "[Epoch 11][Iteration 50][D loss: 0.608997, acc: 75.00%][G loss: 0.862084]\n",
      "[Epoch 11][Iteration 60][D loss: 0.518852, acc: 92.19%][G loss: 0.876649]\n",
      "[Epoch 11][Iteration 70][D loss: 0.564670, acc: 84.38%][G loss: 0.882185]\n",
      "[Epoch 11][Iteration 80][D loss: 0.588684, acc: 81.25%][G loss: 0.752345]\n",
      "[Epoch 11][Iteration 90][D loss: 0.576587, acc: 82.03%][G loss: 0.812933]\n",
      "[Epoch 11][Iteration 100][D loss: 0.538726, acc: 89.84%][G loss: 0.867277]\n",
      "[Epoch 11][Iteration 110][D loss: 0.591506, acc: 80.47%][G loss: 0.870779]\n",
      "[Epoch 11][Iteration 120][D loss: 0.597312, acc: 78.12%][G loss: 0.900111]\n",
      "[Epoch 11][Iteration 130][D loss: 0.616548, acc: 69.53%][G loss: 0.892334]\n",
      "[Epoch 11][Iteration 140][D loss: 0.564348, acc: 88.28%][G loss: 0.715951]\n",
      "[Epoch 11][Iteration 150][D loss: 0.516083, acc: 87.50%][G loss: 0.863657]\n",
      "[Epoch 12][Iteration 0][D loss: 0.665372, acc: 57.03%][G loss: 0.893456]\n",
      "[Epoch 12][Iteration 10][D loss: 0.529510, acc: 88.28%][G loss: 0.780959]\n",
      "[Epoch 12][Iteration 20][D loss: 0.587406, acc: 78.12%][G loss: 0.894423]\n",
      "[Epoch 12][Iteration 30][D loss: 0.648865, acc: 65.62%][G loss: 0.780674]\n",
      "[Epoch 12][Iteration 40][D loss: 0.591311, acc: 75.78%][G loss: 0.888294]\n",
      "[Epoch 12][Iteration 50][D loss: 0.528731, acc: 85.94%][G loss: 0.868638]\n",
      "[Epoch 12][Iteration 60][D loss: 0.549864, acc: 89.84%][G loss: 0.898474]\n",
      "[Epoch 12][Iteration 70][D loss: 0.561862, acc: 82.03%][G loss: 0.942399]\n",
      "[Epoch 12][Iteration 80][D loss: 0.613529, acc: 75.00%][G loss: 0.903586]\n",
      "[Epoch 12][Iteration 90][D loss: 0.618784, acc: 66.41%][G loss: 0.958953]\n",
      "[Epoch 12][Iteration 100][D loss: 0.567826, acc: 84.38%][G loss: 0.974064]\n",
      "[Epoch 12][Iteration 110][D loss: 0.583375, acc: 75.00%][G loss: 0.848929]\n",
      "[Epoch 12][Iteration 120][D loss: 0.548967, acc: 87.50%][G loss: 0.902732]\n",
      "[Epoch 12][Iteration 130][D loss: 0.605406, acc: 82.03%][G loss: 0.837372]\n",
      "[Epoch 12][Iteration 140][D loss: 0.550252, acc: 85.94%][G loss: 0.863888]\n",
      "[Epoch 12][Iteration 150][D loss: 0.566004, acc: 78.12%][G loss: 0.917257]\n",
      "[Epoch 13][Iteration 0][D loss: 0.591273, acc: 78.12%][G loss: 0.869933]\n",
      "[Epoch 13][Iteration 10][D loss: 0.573824, acc: 82.03%][G loss: 0.759023]\n",
      "[Epoch 13][Iteration 20][D loss: 0.686174, acc: 61.72%][G loss: 0.903316]\n",
      "[Epoch 13][Iteration 30][D loss: 0.558988, acc: 81.25%][G loss: 0.852948]\n",
      "[Epoch 13][Iteration 40][D loss: 0.583550, acc: 79.69%][G loss: 0.896813]\n",
      "[Epoch 13][Iteration 50][D loss: 0.611133, acc: 76.56%][G loss: 0.935487]\n",
      "[Epoch 13][Iteration 60][D loss: 0.513668, acc: 89.06%][G loss: 0.862376]\n",
      "[Epoch 13][Iteration 70][D loss: 0.575237, acc: 84.38%][G loss: 0.934839]\n",
      "[Epoch 13][Iteration 80][D loss: 0.595010, acc: 75.00%][G loss: 0.874459]\n",
      "[Epoch 13][Iteration 90][D loss: 0.649668, acc: 65.62%][G loss: 0.901415]\n",
      "[Epoch 13][Iteration 100][D loss: 0.544707, acc: 87.50%][G loss: 0.787698]\n",
      "[Epoch 13][Iteration 110][D loss: 0.581927, acc: 83.59%][G loss: 0.934133]\n",
      "[Epoch 13][Iteration 120][D loss: 0.516193, acc: 89.84%][G loss: 0.901623]\n",
      "[Epoch 13][Iteration 130][D loss: 0.594954, acc: 79.69%][G loss: 0.961216]\n",
      "[Epoch 13][Iteration 140][D loss: 0.513315, acc: 88.28%][G loss: 0.921529]\n",
      "[Epoch 13][Iteration 150][D loss: 0.532145, acc: 86.72%][G loss: 0.942196]\n",
      "[Epoch 14][Iteration 0][D loss: 0.716245, acc: 64.06%][G loss: 0.903493]\n",
      "[Epoch 14][Iteration 10][D loss: 0.557828, acc: 85.94%][G loss: 0.934882]\n",
      "[Epoch 14][Iteration 20][D loss: 0.580764, acc: 79.69%][G loss: 0.877866]\n",
      "[Epoch 14][Iteration 30][D loss: 0.619498, acc: 72.66%][G loss: 0.968914]\n",
      "[Epoch 14][Iteration 40][D loss: 0.556479, acc: 80.47%][G loss: 0.718102]\n",
      "[Epoch 14][Iteration 50][D loss: 0.515398, acc: 88.28%][G loss: 0.961363]\n",
      "[Epoch 14][Iteration 60][D loss: 0.535667, acc: 83.59%][G loss: 0.795708]\n",
      "[Epoch 14][Iteration 70][D loss: 0.707834, acc: 53.12%][G loss: 0.915443]\n",
      "[Epoch 14][Iteration 80][D loss: 0.543619, acc: 87.50%][G loss: 0.861486]\n",
      "[Epoch 14][Iteration 90][D loss: 0.570520, acc: 84.38%][G loss: 0.918121]\n",
      "[Epoch 14][Iteration 100][D loss: 0.563103, acc: 79.69%][G loss: 0.858123]\n",
      "[Epoch 14][Iteration 110][D loss: 0.501025, acc: 92.97%][G loss: 0.898771]\n",
      "[Epoch 14][Iteration 120][D loss: 0.596201, acc: 75.00%][G loss: 0.886405]\n",
      "[Epoch 14][Iteration 130][D loss: 0.604235, acc: 77.34%][G loss: 0.802862]\n",
      "[Epoch 14][Iteration 140][D loss: 0.640558, acc: 69.53%][G loss: 0.805184]\n",
      "[Epoch 14][Iteration 150][D loss: 0.514346, acc: 89.84%][G loss: 0.823632]\n",
      "[Epoch 15][Iteration 0][D loss: 0.565970, acc: 82.03%][G loss: 0.848196]\n",
      "[Epoch 15][Iteration 10][D loss: 0.573971, acc: 78.12%][G loss: 0.801837]\n",
      "[Epoch 15][Iteration 20][D loss: 0.607160, acc: 69.53%][G loss: 0.862296]\n",
      "[Epoch 15][Iteration 30][D loss: 0.585917, acc: 76.56%][G loss: 0.955194]\n",
      "[Epoch 15][Iteration 40][D loss: 0.575330, acc: 78.91%][G loss: 0.887641]\n",
      "[Epoch 15][Iteration 50][D loss: 0.519563, acc: 92.19%][G loss: 0.912517]\n",
      "[Epoch 15][Iteration 60][D loss: 0.545861, acc: 80.47%][G loss: 0.917565]\n",
      "[Epoch 15][Iteration 70][D loss: 0.544951, acc: 88.28%][G loss: 0.898431]\n",
      "[Epoch 15][Iteration 80][D loss: 0.521789, acc: 92.19%][G loss: 0.857924]\n",
      "[Epoch 15][Iteration 90][D loss: 0.554947, acc: 84.38%][G loss: 0.938913]\n",
      "[Epoch 15][Iteration 100][D loss: 0.548084, acc: 88.28%][G loss: 0.883154]\n",
      "[Epoch 15][Iteration 110][D loss: 0.574234, acc: 83.59%][G loss: 0.880710]\n",
      "[Epoch 15][Iteration 120][D loss: 0.590312, acc: 78.12%][G loss: 0.924401]\n",
      "[Epoch 15][Iteration 130][D loss: 0.650248, acc: 67.19%][G loss: 0.942997]\n",
      "[Epoch 15][Iteration 140][D loss: 0.572137, acc: 77.34%][G loss: 0.883668]\n",
      "[Epoch 15][Iteration 150][D loss: 0.580389, acc: 78.12%][G loss: 0.778157]\n",
      "[Epoch 16][Iteration 0][D loss: 0.539456, acc: 82.81%][G loss: 0.807722]\n",
      "[Epoch 16][Iteration 10][D loss: 0.582209, acc: 78.91%][G loss: 0.948351]\n",
      "[Epoch 16][Iteration 20][D loss: 0.559829, acc: 85.16%][G loss: 0.840239]\n",
      "[Epoch 16][Iteration 30][D loss: 0.685590, acc: 57.81%][G loss: 0.743164]\n",
      "[Epoch 16][Iteration 40][D loss: 0.581213, acc: 84.38%][G loss: 0.738662]\n",
      "[Epoch 16][Iteration 50][D loss: 0.596898, acc: 76.56%][G loss: 0.745077]\n",
      "[Epoch 16][Iteration 60][D loss: 0.597705, acc: 74.22%][G loss: 0.992045]\n",
      "[Epoch 16][Iteration 70][D loss: 0.553313, acc: 82.81%][G loss: 0.906296]\n",
      "[Epoch 16][Iteration 80][D loss: 0.573058, acc: 85.16%][G loss: 0.858205]\n",
      "[Epoch 16][Iteration 90][D loss: 0.596072, acc: 75.00%][G loss: 0.812556]\n",
      "[Epoch 16][Iteration 100][D loss: 0.602168, acc: 75.78%][G loss: 0.938346]\n",
      "[Epoch 16][Iteration 110][D loss: 0.559235, acc: 84.38%][G loss: 0.943433]\n",
      "[Epoch 16][Iteration 120][D loss: 0.604512, acc: 78.91%][G loss: 0.926212]\n",
      "[Epoch 16][Iteration 130][D loss: 0.581338, acc: 82.03%][G loss: 0.909027]\n",
      "[Epoch 16][Iteration 140][D loss: 0.557253, acc: 85.16%][G loss: 0.846763]\n",
      "[Epoch 16][Iteration 150][D loss: 0.514560, acc: 89.06%][G loss: 0.844682]\n",
      "[Epoch 17][Iteration 0][D loss: 0.703398, acc: 57.81%][G loss: 0.921619]\n",
      "[Epoch 17][Iteration 10][D loss: 0.694924, acc: 57.81%][G loss: 0.775894]\n",
      "[Epoch 17][Iteration 20][D loss: 0.617393, acc: 71.88%][G loss: 0.818689]\n",
      "[Epoch 17][Iteration 30][D loss: 0.572008, acc: 72.66%][G loss: 0.835573]\n",
      "[Epoch 17][Iteration 40][D loss: 0.535185, acc: 86.72%][G loss: 0.838466]\n",
      "[Epoch 17][Iteration 50][D loss: 0.568338, acc: 83.59%][G loss: 0.877661]\n",
      "[Epoch 17][Iteration 60][D loss: 0.568241, acc: 83.59%][G loss: 0.807848]\n",
      "[Epoch 17][Iteration 70][D loss: 0.576252, acc: 83.59%][G loss: 0.735937]\n",
      "[Epoch 17][Iteration 80][D loss: 0.585150, acc: 79.69%][G loss: 0.985042]\n",
      "[Epoch 17][Iteration 90][D loss: 0.526289, acc: 85.16%][G loss: 0.917636]\n",
      "[Epoch 17][Iteration 100][D loss: 0.495277, acc: 90.62%][G loss: 0.817586]\n",
      "[Epoch 17][Iteration 110][D loss: 0.624587, acc: 64.84%][G loss: 0.953980]\n",
      "[Epoch 17][Iteration 120][D loss: 0.634874, acc: 69.53%][G loss: 0.886740]\n",
      "[Epoch 17][Iteration 130][D loss: 0.576720, acc: 78.91%][G loss: 0.872961]\n",
      "[Epoch 17][Iteration 140][D loss: 0.591143, acc: 79.69%][G loss: 0.906025]\n",
      "[Epoch 17][Iteration 150][D loss: 0.612790, acc: 68.75%][G loss: 0.865320]\n",
      "[Epoch 18][Iteration 0][D loss: 0.517673, acc: 85.16%][G loss: 0.883855]\n",
      "[Epoch 18][Iteration 10][D loss: 0.614863, acc: 69.53%][G loss: 0.870552]\n",
      "[Epoch 18][Iteration 20][D loss: 0.559767, acc: 83.59%][G loss: 0.906993]\n",
      "[Epoch 18][Iteration 30][D loss: 0.614626, acc: 71.88%][G loss: 0.873475]\n",
      "[Epoch 18][Iteration 40][D loss: 0.606917, acc: 75.00%][G loss: 0.927628]\n",
      "[Epoch 18][Iteration 50][D loss: 0.659600, acc: 59.38%][G loss: 0.832497]\n",
      "[Epoch 18][Iteration 60][D loss: 0.595974, acc: 71.09%][G loss: 0.879001]\n",
      "[Epoch 18][Iteration 70][D loss: 0.592545, acc: 76.56%][G loss: 0.877704]\n",
      "[Epoch 18][Iteration 80][D loss: 0.551061, acc: 84.38%][G loss: 0.818421]\n",
      "[Epoch 18][Iteration 90][D loss: 0.548176, acc: 85.94%][G loss: 0.955747]\n",
      "[Epoch 18][Iteration 100][D loss: 0.538882, acc: 89.06%][G loss: 0.825883]\n",
      "[Epoch 18][Iteration 110][D loss: 0.494355, acc: 88.28%][G loss: 0.950648]\n",
      "[Epoch 18][Iteration 120][D loss: 0.566302, acc: 82.03%][G loss: 0.795973]\n",
      "[Epoch 18][Iteration 130][D loss: 0.492350, acc: 92.97%][G loss: 0.856374]\n",
      "[Epoch 18][Iteration 140][D loss: 0.537180, acc: 88.28%][G loss: 0.830244]\n",
      "[Epoch 18][Iteration 150][D loss: 0.618785, acc: 75.78%][G loss: 0.876892]\n",
      "[Epoch 19][Iteration 0][D loss: 0.679328, acc: 70.31%][G loss: 0.883576]\n",
      "[Epoch 19][Iteration 10][D loss: 0.573263, acc: 80.47%][G loss: 0.947958]\n",
      "[Epoch 19][Iteration 20][D loss: 0.498213, acc: 92.19%][G loss: 0.886903]\n",
      "[Epoch 19][Iteration 30][D loss: 0.522020, acc: 84.38%][G loss: 0.920935]\n",
      "[Epoch 19][Iteration 40][D loss: 0.466632, acc: 95.31%][G loss: 0.836168]\n",
      "[Epoch 19][Iteration 50][D loss: 0.512675, acc: 92.97%][G loss: 0.958150]\n",
      "[Epoch 19][Iteration 60][D loss: 0.505559, acc: 88.28%][G loss: 0.838718]\n",
      "[Epoch 19][Iteration 70][D loss: 0.594726, acc: 76.56%][G loss: 0.968349]\n",
      "[Epoch 19][Iteration 80][D loss: 0.583041, acc: 74.22%][G loss: 0.897460]\n",
      "[Epoch 19][Iteration 90][D loss: 0.506595, acc: 90.62%][G loss: 0.817565]\n",
      "[Epoch 19][Iteration 100][D loss: 0.564159, acc: 82.03%][G loss: 0.994105]\n",
      "[Epoch 19][Iteration 110][D loss: 0.512220, acc: 89.84%][G loss: 0.926503]\n",
      "[Epoch 19][Iteration 120][D loss: 0.568110, acc: 82.81%][G loss: 0.891537]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gan = GAN(128, img_size=(64, 64, 3))\n",
    "gan.connect()\n",
    "gan.train(20, 64, sample_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
