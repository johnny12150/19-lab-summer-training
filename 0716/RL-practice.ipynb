{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import utils as np_utils\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "class utilsToos(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    @staticmethod\n",
    "    def processReward(r):\n",
    "        # TODO: 計算 Reward\n",
    "        '''\n",
    "        對收集到的 reward 進行處理\n",
    "\n",
    "        Parameters:\n",
    "            - r (list): 原始收集到的 reward list\n",
    "    \n",
    "        Returns:\n",
    "            - final_r (np.array): 經處理後的 reward list\n",
    "            \n",
    "        Hint:\n",
    "            Suppose Raw reward R = [r_0, r_1, r_2, r_2]\n",
    "            Then the final reward list = [d_0, d_1, d_2, d_3] where\n",
    "                d_0 = r_0 + r_1 + r_2 + r_3\n",
    "            \td_1 = r_0 + r_1 + r_2 + r_3\n",
    "            \td_2 = r_0 + r_1 + r_2 + r_3\n",
    "            \td_3 = r_0 + r_1 + r_2 + r_3\n",
    "        '''\n",
    "        return \n",
    "    \n",
    "    @staticmethod\n",
    "    def getRandomReward(env, EPISODE):\n",
    "        '''\n",
    "        reward curve 的 baseline，action 是隨機選取\n",
    "        \n",
    "        Parameters: \n",
    "            - env (gym): 環境\n",
    "            - EPISODE (int): 回合數\n",
    "            \n",
    "        Returns:\n",
    "            - r (list): 每個回合所得到的總reward\n",
    "        '''\n",
    "        random_reward = []\n",
    "        \n",
    "        for ep in range(EPISODE):\n",
    "            _ = env.reset()\n",
    "            done = False\n",
    "            reward_counter = 0\n",
    "            \n",
    "            while not done:\n",
    "                _, reward, done, _ = env.step(env.action_space.sample())\n",
    "                reward_counter += reward\n",
    "                \n",
    "            random_reward.append(reward_counter)\n",
    "        \n",
    "        return random_reward\n",
    "\n",
    "class Agent():\n",
    "    '''\n",
    "    實作 Policy-based Algo.: REINFROCE\n",
    "    '''\n",
    "    def __init__(self, s_dim, a_dim, n, lr):\n",
    "        '''\n",
    "        Parameters: \n",
    "            - s_dim (int): observation 的維度\n",
    "            - a_dim (int): action 的維度\n",
    "            - n (int): 蒐集 n 個 trajectory 才更新 model\n",
    "            - lr (float): Agent learning rate\n",
    "        '''\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.n = n\n",
    "        self.lr = lr\n",
    "        self.__buildModel()\n",
    "        \n",
    "        # 儲存 n 個回合的 sample 用\n",
    "        self.s_batch = np.empty(n, dtype = object)\n",
    "        self.a_batch = np.empty(n, dtype = object)\n",
    "        self.r_batch = np.empty(n, dtype = object)\n",
    "        \n",
    "        self.batch_counter = 0\n",
    "        \n",
    "    def __buildModel(self):\n",
    "        # TODO: 建立 Agent(只要在input_layer跟output_layer之間加hidden layer就好)\n",
    "        input_layer = Input(shape=(self.s_dim, ))\n",
    "        h = Dense(??)(input_layer)\n",
    "        \n",
    "        output_layer = Dense(self.a_dim, activation='softmax')(h)\n",
    "        self.model = Model(inputs = input_layer, outputs = output_layer)\n",
    "        self.model.compile(loss = 'categorical_crossentropy', \n",
    "                           optimizer = Adam(lr = self.lr))\n",
    "    \n",
    "    def sampleAction(self, s):\n",
    "        # TODO: 讓 Agent sample 一個 action\n",
    "        return\n",
    "    \n",
    "    def storeSample(self, s, a, r):\n",
    "        # TODO: 儲存一個 episode 中所有的(s,a,r)pair\n",
    "    \n",
    "    def fit(self):\n",
    "        S = np.concatenate(self.s_batch)\n",
    "        A = np.concatenate(self.a_batch)\n",
    "        R = np.concatenate(self.r_batch)\n",
    "        \n",
    "        R -= np.mean(R)\n",
    "        R /= (np.std(R)+K.epsilon())\n",
    "#        R += K.epsilon()\n",
    "        \n",
    "        self.model.fit(S, A*R.reshape(-1, 1), batch_size=64, epochs = 10, verbose = 0)\n",
    "#        self.model.fit(S, A, sample_weight = R, epochs = 1, verbose = 0)\n",
    "        \n",
    "        self.batch_counter = 0\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # TODO: 參數設定\n",
    "    EPISODE =  # 要玩幾個回合\n",
    "    N =  # sample 多少個回合才 update model 一次\n",
    "    LR =  # Agent learning rate\n",
    "    RENDER =  # 是否要顯示遊戲畫面\n",
    "    \n",
    "    # 建立環境\n",
    "    env = gym.make('CartPole-v0') \n",
    "    s_dim = env.observation_space.shape[0] # observation 的維度\n",
    "    a_dim = env.action_space.n # action 的維度\n",
    "    \n",
    "    # 建立 Agent\n",
    "    agent = Agent(s_dim, a_dim, N, LR)\n",
    "    \n",
    "    # 取得隨機動作的 reward curve\n",
    "    random_reward = utilsToos.getRandomReward(env, EPISODE)\n",
    "    \n",
    "    # 儲存 Agent 的 reward curve\n",
    "    agent_reward = np.zeros(EPISODE)\n",
    "    \n",
    "    # 儲存 trajectory\n",
    "    s_buffer = np.empty(env.spec.max_episode_steps, dtype = object)\n",
    "    a_buffer = np.empty(env.spec.max_episode_steps, dtype = object)\n",
    "    r_buffer = np.empty(env.spec.max_episode_steps, dtype = object)\n",
    "    \n",
    "    for ep in range(1, EPISODE+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        buffer_counter = 0\n",
    "        reward_counter = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            if RENDER: env.render()\n",
    "            \n",
    "            action = agent.sampleAction(state[None])\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            \n",
    "            reward_counter += reward\n",
    "            \n",
    "            s_buffer[buffer_counter] = state\n",
    "            a_buffer[buffer_counter] = action\n",
    "            r_buffer[buffer_counter] = reward\n",
    "            buffer_counter += 1\n",
    "            \n",
    "            state = state2\n",
    "            \n",
    "            if done: \n",
    "                agent.storeSample(s_buffer[:buffer_counter], \n",
    "                                  a_buffer[:buffer_counter], \n",
    "                                  r_buffer[:buffer_counter])\n",
    "            \n",
    "        agent_reward[ep-1] = reward_counter\n",
    "        \n",
    "        if ep % N == 0: agent.fit()\n",
    "        print('[ep. %.4d] rewards = %.4f'%(ep, reward_counter))\n",
    "        \n",
    "    # 如果有使用 env.render() 記得要呼叫 env.close()\n",
    "    env.close() \n",
    "    \n",
    "    # 把 Agent 跟 basleine 的 reward curve 畫出來\n",
    "    plt.plot(random_reward, label='random choice')\n",
    "    plt.plot(agent_reward, label='policy gradient')\n",
    "    plt.title('reward curve')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('reward')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
