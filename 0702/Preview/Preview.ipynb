{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課前預備/ 預習\n",
    "## 1. Word2Vec - gensim\n",
    "> https://www.kaggle.com/jerrykuo7727/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 語料庫（corpus）、向量（vector）、模型（model）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "在進入 gensim 建模以前，主要進行兩件事情：斷詞、同義字處理。\n",
    "\n",
    "以下先**使用已經斷詞好的csv**，以下是中文的處理方式\n",
    "\n",
    "英文可以參考:\n",
    "> https://ithelp.ithome.com.tw/articles/10191922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "DF = pd.read_csv('title1-8_final.csv')\n",
    "DF.dropna(axis=0, inplace=True)\n",
    "\n",
    "segement = DF['Title_CKIP'].copy()\n",
    "segement = segement.astype(str)\n",
    "\n",
    "segement_arr = []\n",
    "for index in range(segement.size):\n",
    "    segement.values[index] = re.sub(\"\\\\u30001\", \"\", segement.values[index])\n",
    "    segement.values[index] = re.sub(\"\\\\u3000\", \" \", segement.values[index])\n",
    "    segement.values[index] = re.sub(\"\\d+|\\?|\\.|\\:|\\《|\\|》\\？|\\「|\\」|\\！|\\：|\\、|\\【|\\】\", \"\", segement.values[index]) \n",
    "    # 在DF中用array存斷詞\n",
    "    words = segement.values[index].strip().split(' ')\n",
    "    # 移除空字串\n",
    "    words = list(filter(None, words))\n",
    "    segement_arr.append(words)\n",
    "                \n",
    "segement_df = pd.Series(segement_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [捷運, 間隔, 分鐘, 放, 人, 進站, 人潮, 塞爆, 國父, 紀念館, 站]\n",
       "1       [高捷, 延至, 兩, 點收, 班, 夢時代, 跨年, 人潮, 小時, 散去]\n",
       "2           [日, 北, 捷運, 量, 萬, 人, 比, 前, 年少, 萬, 人]\n",
       "3     [北捷, 跨年, 輸運, 減少, 萬, 人次, 是, 因, 假日, 沒, 上班族]\n",
       "4              [元旦, 連假, 收尾, 高鐵, 烏日站, 午後, 湧, 人潮]\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每筆資料須為list格式\n",
    "segement_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下是其他常用的參數：\n",
    "> * sg=0：sg=0時以CBOW來訓練，sg=1時以Skip-gram來訓練\n",
    ">     \n",
    ">     而在特性上，Skip-gram比CBOW通常對低頻詞有更好的訓練效果 \n",
    ">\n",
    ">     基於以上的猜想，我們可以嘗試用Skip-gram來訓練詞向量，看看能否得到更高的準確度\n",
    ">\n",
    "> * window=5：CBOW下決定Word2Vec一次取多少詞來預測中間詞（Skip-gram的狀況是反過來的）\n",
    ">\n",
    ">     需要多少詞才能預測中間詞呢？要讓多少詞的含意來影響中間詞的含意呢？\n",
    ">\n",
    ">     思考完就自己作點實驗吧～（※window只差1就有巨大的影響！請務必微調這個參數）\n",
    "> * min_count=5：出現次數大於等於min_count的詞，才會納入Word2Vec的詞典中\n",
    "> * max_vocab_size=None：Word2Vec的詞典容納上限，出現次數最低的詞會優先被剔除\n",
    ">\n",
    ">     降低詞典的最大詞數，可能讓模型更容易抓到規則（噪音減少了），但也可能更難抓到規則（認識的詞太少）\n",
    "> * hs=0：hs=0時採用Negative Sampling，hs=1時採用Hierarchical Softmax\n",
    "> * negative=5：Negative Sampling的取樣數量，5~20適合小數據，2~5適合大數據\n",
    "> * workers=3：訓練用的線程數量（可以加快訓練速度）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec(segement_df, size=250, iter=10, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型儲存/ 開啟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isfile('word2vec.model'):\n",
    "    model = Word2Vec.load('word2vec.model')\n",
    "    \n",
    "# model_word2vec.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作圖(word2vec投影到2D)\n",
    "若中文顯示有錯誤，可以參考:\n",
    "> https://medium.com/marketingdatascience/%E8%A7%A3%E6%B1%BApython-3-matplotlib%E8%88%87seaborn%E8%A6%96%E8%A6%BA%E5%8C%96%E5%A5%97%E4%BB%B6%E4%B8%AD%E6%96%87%E9%A1%AF%E7%A4%BA%E5%95%8F%E9%A1%8C-f7b3773a889b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# a dict of {word: object of numeric vector}\n",
    "vocab = list(model_word2vec.wv.vocab)\n",
    "# 存字典裡全部的單字\n",
    "X = model_word2vec[vocab]\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "# 存每個單字的x, y座標\n",
    "df_tsne = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df_tsne['x'], df_tsne['y'])\n",
    "# 在圖上顯示詞\n",
    "for word, pos in df_tsne.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 比較詞的相似程度(可以一次代多詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>北捷</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>捷運</td>\n",
       "      <td>0.999960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>站</td>\n",
       "      <td>0.999956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不</td>\n",
       "      <td>0.999955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>機捷</td>\n",
       "      <td>0.999951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>台北</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>營運</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>將</td>\n",
       "      <td>0.999949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>到</td>\n",
       "      <td>0.999948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>一</td>\n",
       "      <td>0.999947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>桃捷</td>\n",
       "      <td>0.999946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   北捷       cos\n",
       "0  捷運  0.999960\n",
       "1   站  0.999956\n",
       "2   不  0.999955\n",
       "3  機捷  0.999951\n",
       "4  台北  0.999950\n",
       "5  營運  0.999950\n",
       "6   將  0.999949\n",
       "7   到  0.999948\n",
       "8   一  0.999947\n",
       "9  桃捷  0.999946"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df\n",
    "\n",
    "most_similar(model_word2vec, ['北捷'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 處理label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder()\n",
    "label_ohe = one_hot.fit_transform(DF[['Category']])\n",
    "label_ohe_array = label_ohe.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將剛訓練好的word_embedding運用在RNN上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(model_word2vec.wv.vocab.items()) + 1, model_word2vec.vector_size))\n",
    "word2idx = {}\n",
    "\n",
    "vocab_list = [(word, model_word2vec.wv[word]) for word, _ in model_word2vec.wv.vocab.items()]\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據 model_word2vec 將辭轉成數字/ index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "        new_corpus.append(new_doc)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確保每筆資料長度相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = text_to_index(segement_df)\n",
    "X_train = pad_sequences(X_train, maxlen=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emebedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0628 16:49:00.412173 19364 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 16:49:08.121798 19364 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0628 16:49:08.140827 19364 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0628 16:49:08.147852 19364 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0628 16:49:08.148853 19364 deprecation_wrapper.py:119] From c:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model_RNN = Sequential()\n",
    "\n",
    "# model_RNN.add(Embedding(output_dim=64, input_dim=len(list(model_word2vec.wv.vocab)), input_length=15))\n",
    "model_RNN.add(embedding_layer)\n",
    "\n",
    "model_RNN.add(SimpleRNN(64))\n",
    "model_RNN.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "model_RNN.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1160 samples, validate on 291 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.1782 - acc: 0.4957 - val_loss: 1.2741 - val_acc: 0.4261\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.0838 - acc: 0.5500 - val_loss: 1.2882 - val_acc: 0.4089\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.0857 - acc: 0.5233 - val_loss: 1.3517 - val_acc: 0.4399\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.0727 - acc: 0.5534 - val_loss: 1.3076 - val_acc: 0.4399\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.0543 - acc: 0.5681 - val_loss: 1.3261 - val_acc: 0.4502\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.0495 - acc: 0.5707 - val_loss: 1.3122 - val_acc: 0.4364\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.0390 - acc: 0.5707 - val_loss: 1.3618 - val_acc: 0.4433\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.0406 - acc: 0.5629 - val_loss: 1.2786 - val_acc: 0.4502\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.0260 - acc: 0.5724 - val_loss: 1.2986 - val_acc: 0.4570\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.0176 - acc: 0.5862 - val_loss: 1.2840 - val_acc: 0.4570\n"
     ]
    }
   ],
   "source": [
    "model_RNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "train_history_RNN = model_RNN.fit(X_train, label_ohe_array, batch_size=30, epochs=10,verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
