{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀資料&切字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# 設定開啟txt的方式\n",
    "lines = open('./Preview/cmn.txt', encoding ='utf8').read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # 切割txt中的每一行英文與中文的間格\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    # 分別存英文與中文成list\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # 切割字元存起來當字典\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算字詞數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典排序            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# 計算編碼器、解碼器的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以dict儲存字典單字及序號\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定編碼器、解碼器input起始值(均為0矩陣)\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Attention Layer\n",
    "> https://github.com/neonbjb/ml-notebooks/blob/master/keras-seq2seq-with-attention/keras_translate_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTMCell, RNN\n",
    "\n",
    "# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n",
    "# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\n",
    "class AttentionLSTMCell(LSTMCell):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attentionMode = False\n",
    "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
    "    \n",
    "    # Build is called to initialize the variables that our cell will use. We will let other Keras\n",
    "    # classes (e.g. \"Dense\") actually initialize these variables.\n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):        \n",
    "        # Converts the input sequence into a sequence which can be matched up to the internal\n",
    "        # hidden state.\n",
    "        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n",
    "        \n",
    "        # Transforms the internal hidden state into something that can be used by the attention\n",
    "        # mechanism.\n",
    "        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n",
    "        \n",
    "        # Transforms the combined hidden state and converted input sequence into a vector of\n",
    "        # probabilities for attention.\n",
    "        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n",
    "        \n",
    "        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n",
    "        # input_shape to reflect this.\n",
    "        batch, input_dim = input_shape[0]\n",
    "        batch, timesteps, context_size = input_shape[-1]\n",
    "        lstm_input = (batch, input_dim + context_size)\n",
    "        \n",
    "        # The LSTMCell superclass expects no constant input, so strip that out.\n",
    "        return super(AttentionLSTMCell, self).build(lstm_input)\n",
    "    \n",
    "    # This must be called before call(). The \"input sequence\" is the output from the \n",
    "    # encoder. This function will do some pre-processing on that sequence which will\n",
    "    # then be used in subsequent calls.\n",
    "    def setInputSequence(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        self.input_seq_shaped = self.dense_constant(input_seq)\n",
    "        self.timesteps = tf.shape(self.input_seq)[-2]\n",
    "    \n",
    "    # This is a utility method to adjust the output of this cell. When attention mode is\n",
    "    # turned on, the cell outputs attention probability vectors across the input sequence.\n",
    "    def setAttentionMode(self, mode_on=False):\n",
    "        self.attentionMode = mode_on\n",
    "        \n",
    "    # This method sets up the computational graph for the cell. It implements the actual logic\n",
    "    # that the model follows.\n",
    "    def call(self, inputs, states, constants):\n",
    "        # Separate the state list into the two discrete state vectors.\n",
    "        # ytm is the \"memory state\", stm is the \"carry state\".\n",
    "        ytm, stm = states\n",
    "        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n",
    "        # input timesteps to perform some calculations on it.\n",
    "        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n",
    "        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n",
    "        # and all encoder states. This will squash the resultant sum down to a vector of size\n",
    "        # [batch,timesteps,1]\n",
    "        # Note: Most sources I encounter use tanh for the activation here. I have found with this dataset\n",
    "        # and this model, relu seems to perform better. It makes the attention mechanism far more crisp\n",
    "        # and produces better translation performance, especially with respect to proper sentence termination.\n",
    "        combined_stm_input = self.dense_transform(\n",
    "            keras.activations.relu(stm_repeated + self.input_seq_shaped))\n",
    "        # Performing a softmax generates a log probability for each encoder output to receive attention.\n",
    "        score_vector = keras.activations.softmax(combined_stm_input, 1)\n",
    "        # In this implementation, we grant \"partial attention\" to each encoder output based on \n",
    "        # it's log probability accumulated above. Other options would be to only give attention\n",
    "        # to the highest probability encoder output or some similar set.\n",
    "        context_vector = K.sum(score_vector * self.input_seq, 1)\n",
    "        \n",
    "        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n",
    "        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n",
    "        inputs = K.concatenate([inputs, context_vector])\n",
    "        \n",
    "        # Call into the super-class to invoke the LSTM math.\n",
    "        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n",
    "        \n",
    "        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n",
    "        if(self.attentionMode):\n",
    "            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n",
    "        else:\n",
    "            return res\n",
    "        \n",
    "# Custom implementation of the Keras LSTM that adds an attention mechanism.\n",
    "# This is implemented by taking an additional input (using the \"constants\" of the\n",
    "# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\n",
    "class LSTMWithAttention(RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        cell = AttentionLSTMCell(units=units)\n",
    "        self.units = units\n",
    "        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n",
    "        \n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        self.timesteps = input_shape[0][-2]\n",
    "        return super(LSTMWithAttention, self).build(input_shape) \n",
    "    \n",
    "    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n",
    "    # for breaking this up into calls into the cell for each step.\n",
    "    # The \"constants\" variable is the key to our implementation. It was specifically added\n",
    "    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n",
    "    def call(self, x, constants, **kwargs):\n",
    "        if isinstance(x, list):\n",
    "            self.x_initial = x[0]\n",
    "        else:\n",
    "            self.x_initial = x\n",
    "        \n",
    "        # The only difference in the LSTM computational graph really comes from the custom\n",
    "        # LSTM Cell that we utilize.\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        self.cell.setInputSequence(constants[0])\n",
    "        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型Enocder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-de2e40be5d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdecoder_att_lstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMWithAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# decoder_LSTM = CuDNNLSTM(HIDDEN_DIM, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_att_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_embedding_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# decoder_outputs, _, _ = decoder_LSTM(decoder_embedding_final, initial_state=[state_h, state_c])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m       \u001b[1;31m# Perform the call with temporarily replaced input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m       \u001b[1;31m# Remove the additional_specs from input spec and keep the rest. It is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m       \u001b[1;31m# important to keep since the input spec was populated by build(), and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[1;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m       \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1882\u001b[0m     \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m     \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-04eafbc5042f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTMWithAttention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# This call is invoked with the entire time sequence. The RNN sub-class is responsible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;31m# allow cell (if layer) to build before we set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;31m# set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-04eafbc5042f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# We will augment the input into LSTMCell by concatenating the context vector. Modify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# input_shape to reflect this.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mlstm_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import CuDNNLSTM, LSTM, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "HIDDEN_DIM=50\n",
    "    \n",
    "encoder_inputs = Input(shape=(None, ), name=\"Encoder_input\")\n",
    "encoder_embedding = Embedding(num_encoder_tokens, 40)(encoder_inputs)\n",
    "encoder_LSTM = CuDNNLSTM(HIDDEN_DIM, return_state=True, name=\"Encoder_LSTM\")\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, ), name=\"Decoder_input\")\n",
    "\n",
    "decoder_embedding = Embedding(num_decoder_tokens, 40)\n",
    "\n",
    "decoder_embedding_final = decoder_embedding(decoder_inputs)\n",
    "decoder_att_lstm = LSTMWithAttention(units=HIDDEN_DIM, return_sequences=True, return_state=True)\n",
    "# decoder_LSTM = CuDNNLSTM(HIDDEN_DIM, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "decoder_outputs, _, _ = decoder_att_lstm(inputs=decoder_embedding_final, constants=encoder_outputs, initial_state=[state_h, state_c])\n",
    "# decoder_outputs, _, _ = decoder_LSTM(decoder_embedding_final, initial_state=[state_h, state_c])\n",
    "\n",
    "dense_layer = Dense(num_decoder_tokens, activation='softmax', name=\"Dense\")\n",
    "# outputs = dense_layer(decoder_outputs)\n",
    "outputs = TimeDistributed(dense_layer)(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1669fe47ccc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb(attdec_inputs), \n\u001b[0;32m     18\u001b[0m                                     \u001b[0mconstants\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattenc_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                                     initial_state=attenc_states)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mattdec_d1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mattdec_d2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_out_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m       \u001b[1;31m# Perform the call with temporarily replaced input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m       \u001b[1;31m# Remove the additional_specs from input spec and keep the rest. It is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m       \u001b[1;31m# important to keep since the input spec was populated by build(), and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[1;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m       \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1882\u001b[0m     \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m     \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-04eafbc5042f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTMWithAttention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# This call is invoked with the entire time sequence. The RNN sub-class is responsible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;31m# allow cell (if layer) to build before we set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;31m# set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wade\\.virtualenvs\\19'_summer_vacation-ib8vnh7u\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-04eafbc5042f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# We will augment the input into LSTMCell by concatenating the context vector. Modify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# input_shape to reflect this.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mlstm_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "vocab_in_size = num_encoder_tokens\n",
    "vocab_out_size = num_decoder_tokens\n",
    "embedding_dim = 40\n",
    "units = 50\n",
    "\n",
    "attenc_inputs = Input(shape=(None,), name=\"attenc_inputs\")\n",
    "attenc_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "attenc_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "attenc_outputs, attstate_h, attstate_c = attenc_lstm(attenc_emb(attenc_inputs))\n",
    "attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "attdec_inputs = Input(shape=(None,))\n",
    "attdec_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "attdec_lstm = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "# Note that the only real difference here is that we are feeding attenc_outputs to the decoder now.\n",
    "attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb(attdec_inputs), \n",
    "                                    constants=attenc_outputs, \n",
    "                                    initial_state=attenc_states)\n",
    "attdec_d1 = Dense(units, activation=\"relu\")\n",
    "attdec_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "attdec_out = attdec_d2(Dropout(rate=.4)(attdec_d1(Dropout(rate=.4)(attdec_lstm_out))))\n",
    "\n",
    "attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "attmodel.compile(optimizer=tf.train.AdamOptimizer(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 4s - loss: 2.1928 - acc: 0.0564 - val_loss: 2.5394 - val_acc: 0.0646\n",
      "Epoch 2/30\n",
      " - 3s - loss: 1.9894 - acc: 0.0664 - val_loss: 2.4957 - val_acc: 0.0706\n",
      "Epoch 3/30\n",
      " - 3s - loss: 1.9589 - acc: 0.0689 - val_loss: 2.4828 - val_acc: 0.0696\n",
      "Epoch 4/30\n",
      " - 3s - loss: 1.9449 - acc: 0.0685 - val_loss: 2.4752 - val_acc: 0.0680\n",
      "Epoch 5/30\n",
      " - 3s - loss: 1.9362 - acc: 0.0710 - val_loss: 2.4694 - val_acc: 0.0729\n",
      "Epoch 6/30\n",
      " - 3s - loss: 1.9298 - acc: 0.0722 - val_loss: 2.4669 - val_acc: 0.0726\n",
      "Epoch 7/30\n",
      " - 3s - loss: 1.9249 - acc: 0.0721 - val_loss: 2.4649 - val_acc: 0.0729\n",
      "Epoch 8/30\n",
      " - 3s - loss: 1.9211 - acc: 0.0720 - val_loss: 2.4644 - val_acc: 0.0739\n",
      "Epoch 9/30\n",
      " - 3s - loss: 1.9179 - acc: 0.0722 - val_loss: 2.4632 - val_acc: 0.0739\n",
      "Epoch 10/30\n",
      " - 3s - loss: 1.9155 - acc: 0.0723 - val_loss: 2.4614 - val_acc: 0.0735\n",
      "Epoch 11/30\n",
      " - 3s - loss: 1.9125 - acc: 0.0724 - val_loss: 2.4592 - val_acc: 0.0740\n",
      "Epoch 12/30\n",
      " - 3s - loss: 1.9103 - acc: 0.0723 - val_loss: 2.4593 - val_acc: 0.0740\n",
      "Epoch 13/30\n",
      " - 3s - loss: 1.9075 - acc: 0.0725 - val_loss: 2.4590 - val_acc: 0.0740\n",
      "Epoch 14/30\n",
      " - 3s - loss: 1.9050 - acc: 0.0727 - val_loss: 2.4604 - val_acc: 0.0725\n",
      "Epoch 15/30\n",
      " - 3s - loss: 1.9025 - acc: 0.0729 - val_loss: 2.4597 - val_acc: 0.0728\n",
      "Epoch 16/30\n",
      " - 3s - loss: 1.9001 - acc: 0.0728 - val_loss: 2.4632 - val_acc: 0.0725\n",
      "Epoch 17/30\n",
      " - 3s - loss: 1.8981 - acc: 0.0727 - val_loss: 2.4604 - val_acc: 0.0728\n",
      "Epoch 18/30\n",
      " - 3s - loss: 1.8959 - acc: 0.0727 - val_loss: 2.4619 - val_acc: 0.0725\n",
      "Epoch 19/30\n",
      " - 3s - loss: 1.8943 - acc: 0.0729 - val_loss: 2.4616 - val_acc: 0.0725\n",
      "Epoch 20/30\n",
      " - 3s - loss: 1.8924 - acc: 0.0727 - val_loss: 2.4622 - val_acc: 0.0740\n",
      "Epoch 21/30\n",
      " - 3s - loss: 1.8908 - acc: 0.0728 - val_loss: 2.4631 - val_acc: 0.0720\n",
      "Epoch 22/30\n",
      " - 3s - loss: 1.8893 - acc: 0.0728 - val_loss: 2.4649 - val_acc: 0.0728\n",
      "Epoch 23/30\n",
      " - 3s - loss: 1.8882 - acc: 0.0725 - val_loss: 2.4689 - val_acc: 0.0728\n",
      "Epoch 24/30\n",
      " - 3s - loss: 1.8865 - acc: 0.0726 - val_loss: 2.4689 - val_acc: 0.0725\n",
      "Epoch 25/30\n",
      " - 3s - loss: 1.8853 - acc: 0.0729 - val_loss: 2.4705 - val_acc: 0.0721\n",
      "Epoch 26/30\n",
      " - 3s - loss: 1.8841 - acc: 0.0724 - val_loss: 2.4717 - val_acc: 0.0725\n",
      "Epoch 27/30\n",
      " - 3s - loss: 1.8825 - acc: 0.0730 - val_loss: 2.4720 - val_acc: 0.0715\n",
      "Epoch 28/30\n",
      " - 3s - loss: 1.8815 - acc: 0.0727 - val_loss: 2.4731 - val_acc: 0.0714\n",
      "Epoch 29/30\n",
      " - 3s - loss: 1.8802 - acc: 0.0727 - val_loss: 2.4775 - val_acc: 0.0728\n",
      "Epoch 30/30\n",
      " - 3s - loss: 1.8790 - acc: 0.0728 - val_loss: 2.4768 - val_acc: 0.0729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d363642d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=30, epochs=10, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
    "    decoder_emb(inf_decoder_inputs), \n",
    "    initial_state=[state_input_h, state_input_c])\n",
    "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_input (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 40)          2920      \n",
      "_________________________________________________________________\n",
      "Encoder_LSTM (CuDNNLSTM)     [(None, 50), (None, 50),  18400     \n",
      "=================================================================\n",
      "Total params: 21,320\n",
      "Trainable params: 21,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定義編碼器取樣模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 40)     86680       Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (CuDNNLSTM)        [(None, None, 50), ( 18400       embedding_4[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, None, 2167)   110517      Decoder_LSTM[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 215,597\n",
      "Trainable params: 215,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定義解碼器的input\n",
    "state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "attenc_seq_out = Input(shape=attenc_outputs.get_shape()[1:], name=\"attenc_seq_out\")\n",
    "inf_attdec_inputs = Input(shape=(None,), name=\"inf_attdec_inputs\")\n",
    "attdec_lstm.cell.setAttentionMode(True)\n",
    "\n",
    "# 做embedding\n",
    "decoder_embedding_final2 = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# 定義解碼器 LSTM 模型\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding_final2, initial_state=decoder_states_inputs)\n",
    "attdec_res, attdec_h, attdec_c = attdec_lstm(decoder_embedding_final2, \n",
    "                                                initial_state=[state_input_h, state_input_c], \n",
    "                                                constants=attenc_seq_out)\n",
    "\n",
    "# 以編碼器的記憶狀態 h 及 c 為解碼器的記憶狀態  \n",
    "# decoder_states2 = [state_h2, state_c2]\n",
    "# decoder_outputs2 = dense_layer(decoder_outputs2)\n",
    "# decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "decoder_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], outputs=[attdec_res, attdec_h, attdec_c])\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立反向的 dict，才能透過查詢將數值轉回文字\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻譯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, lang):\n",
    "    pre = preprocess_sentence(sentence)\n",
    "    vec = np.zeros(len_input)\n",
    "    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n",
    "    for i,w in enumerate(sentence_list):\n",
    "        vec[i] = w\n",
    "    return vec\n",
    "\n",
    "def translate(input_sentence, infenc_model, infmodel, attention=False):\n",
    "    sv = sentence_to_vector(input_sentence, input_lang)\n",
    "    # Reshape so we can use the encoder model. New shape=[samples,sequence length]\n",
    "    sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = target_lang.word2idx[\"<start>\"]\n",
    "    stop_vec = target_lang.word2idx[\"<end>\"]\n",
    "    # We will continuously feed cur_vec as an input into the decoder to produce the next word,\n",
    "    # which will be assigned to cur_vec. Start it with \"<start>\".\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "    # Start doing the feeding. Terminate when the model predicts an \"<end>\" or we reach the end\n",
    "    # of the max target language sentence length.\n",
    "    while cur_word != \"<end>\" and i < (len_target-1):\n",
    "        i += 1\n",
    "        if cur_word != \"<start>\":\n",
    "            output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        # This will allow us to accomodate attention models, which we will talk about later.\n",
    "        if attention:\n",
    "            x_in += [emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        # The output of the model is a massive softmax vector with one spot for every possible word. Convert\n",
    "        # it to a word ID using argmax().\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Hi.\", encoder_model, inf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型預測，並取得翻譯結果(中文)    \n",
    "# def decode_sequence(input_seq):\n",
    "#     # Encode the input as state vectors.\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "#     # Generate empty target sequence of length 1.\n",
    "#     target_seq = np.zeros((1, 1))\n",
    "    \n",
    "#     # Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0] = 1.\n",
    "    \n",
    "#     # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "#     stop_condition = False\n",
    "#     decoded_sentence = ''\n",
    "#     while not stop_condition:\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "#         # Sample a token\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "#         decoded_sentence += sampled_char\n",
    "\n",
    "#         # Exit condition: either hit max length or find stop character.\n",
    "#         if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "#             stop_condition = True\n",
    "\n",
    "#         # Update the target sequence (of length 1).\n",
    "#         target_seq = np.zeros((1, 1))\n",
    "#         target_seq[0, 0] = 1.\n",
    "\n",
    "#         # Update states\n",
    "#         states_value = [h, c]\n",
    "\n",
    "#     return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 我。。。\n",
      "\n",
      "*\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 我姆。。\n",
      "\n",
      "*\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 我姆。。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for seq_index in range(30):\n",
    "#     # Take one sequence (part of the training test) for trying out decoding.\n",
    "#     input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print('*')\n",
    "#     print('Input sentence:', input_texts[seq_index])\n",
    "#     try:\n",
    "#         print('Decoded sentence:', decoded_sentence)\n",
    "#     except:\n",
    "#         # 出現亂碼，以?取代\n",
    "#         print('Decoded sentence:', decoded_sentence.encode('ascii', 'replace'))\n",
    "#         #print(\"error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
