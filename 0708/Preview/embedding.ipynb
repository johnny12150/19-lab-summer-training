{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預習\n",
    "## 1. Keras LSTM Seq2Seq\n",
    "> http://www.zmonster.me/2016/05/29/sequence_to_sequence_with_keras.html\n",
    "\n",
    "Encoder 只在序列結束時輸出一個語義向量，所以其\"return_sequences\" 參數設置為\"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取txt資料\n",
    "> https://stackoverflow.com/questions/21546739/load-data-from-txt-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>嗨。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>你好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>你用跑的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等等！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>你好。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       en     zh\n",
       "0     Hi.     嗨。\n",
       "1     Hi.    你好。\n",
       "2    Run.  你用跑的。\n",
       "3   Wait!    等等！\n",
       "4  Hello!    你好。"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 中英文的間距是一個tab\n",
    "DF = pd.read_csv('cmn.txt', sep=\"\t\", header=None)\n",
    "DF.columns = ['en', 'zh']\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    0\n",
       "zh    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 另一種讀txt取文字的方式，順便建立字典\n",
    "> https://ithelp.ithome.com.tw/articles/10194403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# 設定開啟txt的方式\n",
    "lines = open('cmn.txt', encoding ='utf8').read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # 切割txt中的每一行英文與中文的間格\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    # 分別存英文與中文成list\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # 切割'字元'存起來當字典\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典排序            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# 計算編碼器、解碼器的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以dict儲存字典單字及序號\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "2167\n"
     ]
    }
   ],
   "source": [
    "print(len(input_token_index))\n",
    "print(len(target_token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定編碼器、解碼器input起始值(均為0矩陣)\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 encoder_input、decoder_input對應的順序    \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize/ Make Vocabulary \n",
    "\n",
    "**中文跟英文的應該要分開**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def vocab_creater(text_lists):\n",
    "\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(text_lists)\n",
    "  dictionary = tokenizer.word_index\n",
    "  \n",
    "  word2idx = {}\n",
    "  idx2word = {}\n",
    "  for k, v in dictionary.items():\n",
    "          word2idx[k] = v\n",
    "          idx2word[v] = k\n",
    "          \n",
    "  return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = vocab_creater(text_lists=DF['en'].values + DF['zh'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def text2seq(encoder_text, decoder_text):\n",
    "\n",
    "  tokenizer = Tokenizer()\n",
    "  encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
    "  decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
    "  \n",
    "  return encoder_sequences, decoder_sequences\n",
    "\n",
    "encoder_sequences, decoder_sequences = text2seq(DF['en'].values, DF['zh'].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "> https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639\n",
    "\n",
    "> https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 做shared embedding\n",
    "> https://stackoverflow.com/questions/49477097/keras-seq2seq-word-embedding\n",
    "\n",
    "#### return_seq and reture_state\n",
    "> https://blog.csdn.net/u011327333/article/details/78501054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 40)     2920        Encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 40)     86680       Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder_LSTM (CuDNNLSTM)        [(None, 50), (None,  18400       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (CuDNNLSTM)        [(None, None, 50), ( 18400       embedding_4[0][0]                \n",
      "                                                                 Encoder_LSTM[0][1]               \n",
      "                                                                 Encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, None, 2167)   110517      Decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 236,917\n",
      "Trainable params: 236,917\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import CuDNNLSTM, LSTM, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "HIDDEN_DIM=50\n",
    "    \n",
    "# encoder_inputs = Input(shape=(max_encoder_seq_length, ), name=\"Encoder_input\")\n",
    "encoder_inputs = Input(shape=(None, ), name=\"Encoder_input\")\n",
    "\n",
    "# encoder_embedding = Embedding(len(input_token_index), 40, input_length=max_encoder_seq_length)\n",
    "# encoder_embedding = Embedding(len(input_token_index), 40)\n",
    "encoder_embedding = Embedding(num_encoder_tokens, 40)(encoder_inputs)\n",
    "\n",
    "encoder_LSTM = CuDNNLSTM(HIDDEN_DIM, return_state=True, name=\"Encoder_LSTM\")\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# decoder_inputs = Input(shape=(max_decoder_seq_length, ), name=\"Decoder_input\")\n",
    "decoder_inputs = Input(shape=(None, ), name=\"Decoder_input\")\n",
    "\n",
    "# decoder_embedding = Embedding(len(target_token_index), 40, input_length=max_decoder_seq_length)\n",
    "# decoder_embedding =  Embedding(len(target_token_index), 40)\n",
    "decoder_embedding = Embedding(num_decoder_tokens, 40)\n",
    "\n",
    "decoder_embedding_final = decoder_embedding(decoder_inputs)\n",
    "decoder_LSTM = CuDNNLSTM(HIDDEN_DIM, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding_final, initial_state=[state_h, state_c])\n",
    "\n",
    "dense_layer = Dense(num_decoder_tokens, activation='softmax', name=\"Dense\")\n",
    "outputs = dense_layer(decoder_outputs)\n",
    "# outputs = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1527 - acc: 0.0601 - val_loss: 2.5232 - val_acc: 0.0640\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9444 - acc: 0.0674 - val_loss: 2.4736 - val_acc: 0.0702\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9060 - acc: 0.0696 - val_loss: 2.4511 - val_acc: 0.0688\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.8902 - acc: 0.0701 - val_loss: 2.4441 - val_acc: 0.0709\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.8812 - acc: 0.0724 - val_loss: 2.4455 - val_acc: 0.0716\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8739 - acc: 0.0726 - val_loss: 2.4362 - val_acc: 0.0728\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8687 - acc: 0.0728 - val_loss: 2.4290 - val_acc: 0.0720\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8650 - acc: 0.0731 - val_loss: 2.4303 - val_acc: 0.0729\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8620 - acc: 0.0728 - val_loss: 2.4310 - val_acc: 0.0730\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.8591 - acc: 0.0730 - val_loss: 2.4253 - val_acc: 0.0720\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8570 - acc: 0.0729 - val_loss: 2.4237 - val_acc: 0.0726\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8549 - acc: 0.0729 - val_loss: 2.4230 - val_acc: 0.0720\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8530 - acc: 0.0730 - val_loss: 2.4217 - val_acc: 0.0720\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.8517 - acc: 0.0730 - val_loss: 2.4220 - val_acc: 0.0725\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8500 - acc: 0.0732 - val_loss: 2.4280 - val_acc: 0.0723\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8489 - acc: 0.0731 - val_loss: 2.4241 - val_acc: 0.0720\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8475 - acc: 0.0731 - val_loss: 2.4228 - val_acc: 0.0726\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8465 - acc: 0.0729 - val_loss: 2.4224 - val_acc: 0.0718\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8452 - acc: 0.0731 - val_loss: 2.4301 - val_acc: 0.0726\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8441 - acc: 0.0732 - val_loss: 2.4258 - val_acc: 0.0718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2505f73d320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=300, epochs=40, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測(翻譯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder_input (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 40)          2920      \n",
      "_________________________________________________________________\n",
      "Encoder_LSTM (CuDNNLSTM)     [(None, 50), (None, 50),  18400     \n",
      "=================================================================\n",
      "Total params: 21,320\n",
      "Trainable params: 21,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定義編碼器取樣模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 40)     86680       Decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_LSTM (CuDNNLSTM)        [(None, None, 50), ( 18400       embedding_4[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dense (Dense)                   (None, None, 2167)   110517      Decoder_LSTM[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 215,597\n",
      "Trainable params: 215,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定義解碼器的input\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 做embedding\n",
    "decoder_embedding_final2 = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# 定義解碼器 LSTM 模型\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding_final2, initial_state=decoder_states_inputs)\n",
    "# decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# 以編碼器的記憶狀態 h 及 c 為解碼器的記憶狀態  \n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = dense_layer(decoder_outputs2)\n",
    "# decoder_model = Model([decoder_embedding] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立反向的 dict，才能透過查詢將數值轉回文字\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型預測，並取得翻譯結果(中文)    \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    target_seq[0, 0] = 1.\n",
    "    \n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 我。。\n",
      "\n",
      "*\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 我。。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test) for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('*')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    try:\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "    except:\n",
    "        # 出現亂碼，以?取代\n",
    "        print('Decoded sentence:', decoded_sentence.encode('ascii', 'replace'))\n",
    "        #print(\"error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
